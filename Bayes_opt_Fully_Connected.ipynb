{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bayesian-optimization --user\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./../credit44_sc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df.iloc[:, :-1].values.astype(np.float32)\n",
    "y_datalabel = df.iloc[:, -1]\n",
    "y_data = LabelEncoder().fit_transform(df.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.zeros((y_data.shape[0], np.unique(y_data).shape[0]))\n",
    "for i in range(y_data.shape[0]):\n",
    "    onehot[i, y_data[i]] = 1.0\n",
    "    \n",
    "x_train, x_test, y_train, y_test, y_train_label, y_test_label = train_test_split(x_data, onehot, y_data, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(num_hidden, size_layer, learning_rate , dropout_rate , beta , activation , batch_size = 16):\n",
    "    \n",
    "    def activate(activation, first_layer, second_layer, bias):\n",
    "        if activation == 0:\n",
    "            activation = tf.nn.leaky_relu\n",
    "        elif activation == 1:\n",
    "            activation = tf.nn.tanh\n",
    "        else:\n",
    "            activation = tf.nn.relu\n",
    "        layer = activation(tf.matmul(first_layer, second_layer) + bias)\n",
    "        return tf.nn.dropout(layer, dropout_rate)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (None, x_data.shape[1]))\n",
    "    Y = tf.placeholder(tf.float32, (None, onehot.shape[1]))\n",
    "    ## W = tf.Variable(tf.contrib.layers.xavier_initializer()((n_prev, n)))\n",
    "    \n",
    "    input_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((x_data.shape[1], size_layer)))\n",
    "    biased_layer = tf.Variable(tf.random_normal([size_layer], stddev = 0.1))\n",
    "    output_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer, onehot.shape[1])))\n",
    "    biased_output = tf.Variable(tf.random_normal([onehot.shape[1]], stddev = 0.1))\n",
    "    \n",
    "    \n",
    "    layers, biased = [], []\n",
    "    \n",
    "    for i in range(num_hidden - 1):\n",
    "        layers.append(tf.Variable(tf.random_normal([size_layer, size_layer])))\n",
    "        biased.append(tf.Variable(tf.random_normal([size_layer])))\n",
    "    \n",
    "    first_l = activate(activation, X, input_layer, biased_layer)\n",
    "    next_l = activate(activation, first_l, layers[0], biased[0])\n",
    "    \n",
    "    for i in range(1, num_hidden - 1):\n",
    "        next_l = activate(activation, next_l, layers[i], biased[i])\n",
    "    \n",
    "    last_l = tf.matmul(next_l, output_layer) + biased_output\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = last_l, labels = Y))\n",
    "    regularizers = tf.nn.l2_loss(input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n",
    "    cost = cost + beta * regularizers\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(last_l, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n",
    "    for i in range(100):\n",
    "        train_acc, train_loss = 0, 0\n",
    "        for n in range(0, (x_train.shape[0] // batch_size) * batch_size, batch_size):\n",
    "            _, loss = sess.run([optimizer, cost], feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n",
    "            train_acc += sess.run(accuracy, feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n",
    "            train_loss += loss\n",
    "        TEST_COST.append(sess.run(cost, feed_dict = {X: x_test, Y: y_test}))\n",
    "        TEST_ACC.append(sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))\n",
    "        train_loss /= (x_train.shape[0] // batch_size)\n",
    "        train_acc /= (x_train.shape[0] // batch_size)\n",
    "        ACC.append(train_acc)\n",
    "        COST.append(train_loss)\n",
    "    COST = np.array(COST).mean()\n",
    "    TEST_COST = np.array(TEST_COST).mean()\n",
    "    ACC = np.array(ACC).mean()\n",
    "    TEST_ACC = np.array(TEST_ACC).mean()\n",
    "    return COST, TEST_COST, ACC, TEST_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nn(num_hidden, size_layer, learning_rate, dropout_rate, beta, activation):\n",
    "    global accbest\n",
    "    param = {\n",
    "        'num_hidden' : int(np.around(num_hidden)),\n",
    "        'size_layer' : int(np.around(size_layer)),\n",
    "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n",
    "        'dropout_rate' : max(min(dropout_rate, 0.99), 0),\n",
    "        'beta' : max(min(beta, 0.5), 0.000001),\n",
    "        'activation': int(np.around(activation))\n",
    "    }\n",
    "    print(\"\\nSearch parameters %s\" % (param), file = log_file)\n",
    "    log_file.flush()\n",
    "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n",
    "    print(\"stop after 200 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n",
    "    if (valid_acc > accbest):\n",
    "        costbest = valid_acc\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... |   beta    | dropou... | learni... | num_hi... | size_l... |\n",
      "-------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "log_file = open('nn-bayesian.log', 'a')\n",
    "accbest = 0.0\n",
    "NN_BAYESIAN = BayesianOptimization(generate_nn, \n",
    "                              {'num_hidden': (2, 20),\n",
    "                               'size_layer': (32, 1024),\n",
    "                               'learning_rate': (0.0001, 1),\n",
    "                               'dropout_rate': (0.1, 0.99),\n",
    "                               'beta': (0.000001, 0.49),\n",
    "                               'activation': (0, 2)\n",
    "                              })\n",
    "NN_BAYESIAN.maximize(init_points = 30, n_iter = 50, acq = 'ei', xi = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\n",
    "print('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
