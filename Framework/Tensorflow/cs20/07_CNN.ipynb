{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_CNN.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"IuqQHNqo1Pgc","colab_type":"text"},"cell_type":"markdown","source":["# 2D Convolution \n","\n","```\n","tf.nn.conv2d(\n","    input,\n","    filter,\n","    strides,\n","    padding,\n","    use_cudnn_on_gpu=True,\n","    data_format='NHWC',\n","    dilations=[1, 1, 1, 1],\n","    name=None )\n","    \n","    \n","Input: Batch size (N) x Height (H) x Width (W) x Channels (C)\n","\n","Filter: Height x Width x Input Channels x Output Channels\n","(e.g. [5, 5, 3, 64])\n","Strides: 4 element 1-D tensor, strides in each direction\n","(often [1, 1, 1, 1] or [1, 2, 2, 1])\n","\n","Padding: 'SAME' or 'VALID'\n","Dilations: The dilation factor. If set to k > 1, there will be k-1 skipped cells between each filter element on that dimension.\n","Data_format: default to NHWC\n","\n","```\n","\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"51JJVcvN1svd","colab_type":"text"},"cell_type":"markdown","source":["## Convolutional layer\n","\n","## A common practice is to group convolutional layer and non-linearity together, which we will do in this case\n","\n","the input volume size (W)\n","\n","the receptive field size of filter (F)\n","\n","the stride with which they are applied (S)\n","\n","the amount of zero padding used (P) on the border. \n","\n","\n","## (W-F + 2P) / S + 1"]},{"metadata":{"id":"umDccnje1JKR","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def conv_relu(inputs , filters ,k_size1 ,k_size2, stride , padding , scope_name) :\n","  with tf.variable_scope(scope_name , reuse=tf.AUTO_RESUE) as scope :\n","    in_channels = inputs.shape[-1]\n","    kernel = tf.get_variable(\"kernel\" , [k_size_1, k_size_2, in_channels , filters],\n","                            initializer = tf.truncated_normal_initializer())\n","    biases =tf.get_variable(\"biases\" ,[filters] , initializer=tf.random_normal_initializer())\n","    conv = tf.nn.conv2d(inputs, kernel , strides=[1,stride, stride , 1] ,padding=padding)\n","    return tf.nn.relu(conv+bias ,name= scope_name)\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Wk46DC_03ZC9","colab_type":"text"},"cell_type":"markdown","source":["# Pooling\n","##  Pooling is a downsampling technique to reduce the dimensionality of the feature map extracted from the convolutional layer in order to reduce the processing time.\n","\n","We can compute the spatial size on each dimension (width/depth) of the output of pooling layer a function of:\n","\n","the input volume size (W)\n","\n","the pooling size (K)\n","\n","the stride with which they are applied (S)\n","\n","the amount of zero padding used (P) on the border. \n","\n","The formula is as followed:\n","\n","## (W-K + 2P) / S + 1\n"]},{"metadata":{"id":"_4vDAcx92oO4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def maxpool(inputs, ksize, stride, padding = \"VALID\" , scope_name=\"pool\") :\n","  with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope :\n","    pool = tf.nn.max_pool(inputs , ksize= [1,ksize, ksize ,1], \n","                         strides = [1,stride, stride, 1],\n","                         padding=padding)\n","    return pool\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"2hWYvcqP3mC5","colab_type":"text"},"cell_type":"markdown","source":["# Fully connected\n","\n","### Fully connected, or dense, layer is called so because every node in the layer is connected to every node in the preceding layer.\n","\n","### Convolutional layers are only locally connected.\n"]},{"metadata":{"id":"cAoUyycN3VUA","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def fully_connected(inputs, out_dim , scope_name=\"fc\") :\n","  with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope :\n","    in_dim = inputs.shape[-1]\n","    w= tf.get_variable(\"weights\" ,[in_dim ,out_dim] , \n","                      initializer=tf.truncated_normal_initializer())\n","    b = tf.get_variable(\"biases\" ,[out_dim] , initializer = tf.constant_initializer(0.0))\n","    \n","    out=tf.matmul(inputs,w)+b\n","    return out"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ix4vyUAJ4O14","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def inference(self):\n","        conv1 = conv_relu(inputs=self.img,\n","                        filters=32,\n","                        k_size=5,\n","                        stride=1,\n","                        padding='SAME',\n","                        scope_name='conv1')\n","        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')  \n","        conv2 = conv_relu(inputs=pool1,                     # reuse=tf.AUTO_REUSE 효과 굳굳\n","                        filters=64,\n","                        k_size=5,\n","                        stride=1,\n","                        padding='SAME',\n","                        scope_name='conv2')\n","        pool2 = maxpool(conv2, 2, 2, 'VALID', 'pool2')  # reuse=tf.AUTO_REUSE\n","        feature_dim = pool2.shape[1] * pool2.shape[2] * pool2.shape[3]\n","        pool2 = tf.reshape(pool2, [-1, feature_dim])\n","        fc = tf.nn.relu(fully_connected(pool2, 1024, 'fc'))\n","        dropout = tf.layers.dropout(fc, self.keep_prob, training=self.training, name='dropout')\n","        \n","        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n","                                                         \n","  \n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"oJx229v2-IBn","colab_type":"text"},"cell_type":"markdown","source":["\"VALID\" only ever drops the right-most columns (or bottom-most rows).\n","\n","\"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right, as is the case in this example (the same logic applies vertically: there may be an extra row of zeros at the bottom)."]},{"metadata":{"id":"M3JFVFjY7Twy","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["def eval(self) :\n","        '''\n","        Count the number of right predictions in a batch\n","        '''\n","    with tf.name_scope(\"predict\") :\n","      preds =tf.nn.softmax(self.logits)\n","      correct_preds =tf.equal(tf.argmax(preds,1),tf.argmax(self.label,1))\n","      self.accuracy= tf.reduce_sum(tf.cast(correct_preds, ft.float32))\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Z2KOPaSC5mMQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","a= np.random.standard_normal(size=[1000]).reshape(10,10,2,5)\n","print(a.shape)\n","z = a.shape[1]*a.shape[2]*a.shape[3]\n","k=np.reshape(a, [-1, z])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"x7mNPCPw7rwQ","colab_type":"text"},"cell_type":"markdown","source":["#   tf.layers\n","## I have  a confession to make: we’ve been learning how to do it the hard way.\n","## We actually don’t have to write the conv_relu method, pooling, or fully_connected.\n","##  tf.layers that provides many off-the-shelf layers. Higher level libraries like Keras, Sonnet also provide ready-made models that you can call with a few lines  of code. \n","\n","```\n","conv1 = tf.layers.conv2d(inputs=self.img,\n","                                  filters=32,\n","                                  kernel_size=[5, 5],\n","                                  padding='SAME',\n","                                  activation=tf.nn.relu,\n","                                  name='conv1')\n","\n","pool1 = tf.layers.max_pooling2d(inputs=conv1, \n","                                        pool_size=[2, 2], \n","                                        strides=2,\n","                                        name='pool1')\n","\n","fc = tf.layers.dense(pool2, 1024, activation=tf.nn.relu, name='fc')\n","\n","dropout = tf.layers.dropout(fc, \n","                                    self.keep_prob, \n","                                    training=self.training, \n","                                    name='dropout')\n","                                  \n","```\n","It’s pretty straightforward to use tf.layers. \n","\n","There’s only one tiny thing to note. With tf.layers.dropout, you need to have another variable to indicate whether it’s in the training mode or in the evaluation mode. \n","\n","We want to drop out neurons during training, but we want to use all of them during evaluating!\n","\n","Drop neurals during training\n","Want to use all of them during testing\n","\n","\n"]},{"metadata":{"id":"xWt80etU5xLQ","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}