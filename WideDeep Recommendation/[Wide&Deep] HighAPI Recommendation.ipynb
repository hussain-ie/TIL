{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy as cp\n",
    "import Agile_data\n",
    "import os , re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "import copy as cp\n",
    "#tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, test_categorical, test_continuous, test_Segment, \\\n",
    "test_label , test_label_eval  = Agile_data.load_test_data()\n",
    "_, Deep_data_test, test_label = Agile_data.load_Wide_Deep_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변형한 데이터는 ㄷㄷ 똑같음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = pd.read_csv(\"Total_Data_ver2.csv\" , nrows = 1)\n",
    "test_ = pd.read_csv(\"Test_Data_ver2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = Input.columns.tolist()\n",
    "Continuous_col1 = [ i for i in columns if re.search(\"D2V\" , i) is not None ]\n",
    "Continuous_col2 = [ i for i in columns if re.search(\"[A-D][0-9]{2}\" , i) is not None ]\n",
    "Continuous_col3 = [\"dadungi\" , \"Deoyoung\" ,\"Robs\", \"himart\"]\n",
    "Categorical_Seg = [ i for i in columns if re.search(\"^X\" , i) is not None ]\n",
    "Categorical_col = [\"Gender\" , \"AgeGroup\" ,\"A_residential_area\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert test_.columns.tolist() + [\"target\"] == Input.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = Continuous_col1 + Categorical_col + Continuous_col3 + Continuous_col2 + Categorical_Seg\n",
    "assert len(Input.columns) == len(columns) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous_col = cp.deepcopy(Continuous_col1)\n",
    "Continuous_col.extend(Continuous_col2) \n",
    "Continuous_col.extend(Continuous_col3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D2V_0', 'D2V_1', 'D2V_2', 'D2V_3', 'D2V_4', 'D2V_5', 'D2V_6', 'D2V_7', 'D2V_8', 'D2V_9', 'D2V_10', 'D2V_11', 'D2V_12', 'D2V_13', 'D2V_14', 'D2V_15', 'D2V_16', 'D2V_17', 'D2V_18', 'D2V_19', 'D2V_20', 'D2V_21', 'D2V_22', 'D2V_23', 'D2V_24', 'D2V_25', 'D2V_26', 'D2V_27', 'D2V_28', 'D2V_29', 'D2V_30', 'D2V_31', 'D2V_32', 'D2V_33', 'D2V_34', 'D2V_35', 'D2V_36', 'D2V_37', 'D2V_38', 'D2V_39', 'D2V_40', 'D2V_41', 'D2V_42', 'D2V_43', 'D2V_44', 'D2V_45', 'D2V_46', 'D2V_47', 'D2V_48', 'D2V_49', 'D2V_50', 'D2V_51', 'D2V_52', 'D2V_53', 'D2V_54', 'D2V_55', 'D2V_56', 'D2V_57', 'D2V_58', 'D2V_59', 'D2V_60', 'D2V_61', 'D2V_62', 'D2V_63', 'D2V_64', 'D2V_65', 'D2V_66', 'D2V_67', 'D2V_68', 'D2V_69', 'D2V_70', 'D2V_71', 'D2V_72', 'D2V_73', 'D2V_74', 'D2V_75', 'D2V_76', 'D2V_77', 'D2V_78', 'D2V_79', 'D2V_80', 'D2V_81', 'D2V_82', 'D2V_83', 'D2V_84', 'D2V_85', 'D2V_86', 'D2V_87', 'D2V_88', 'D2V_89', 'D2V_90', 'D2V_91', 'D2V_92', 'D2V_93', 'D2V_94', 'D2V_95', 'D2V_96', 'D2V_97', 'D2V_98', 'D2V_99', 'D2V_100', 'D2V_101', 'D2V_102', 'D2V_103', 'D2V_104', 'D2V_105', 'D2V_106', 'D2V_107', 'D2V_108', 'D2V_109', 'D2V_110', 'D2V_111', 'D2V_112', 'D2V_113', 'D2V_114', 'D2V_115', 'D2V_116', 'D2V_117', 'D2V_118', 'D2V_119', 'D2V_120', 'D2V_121', 'D2V_122', 'D2V_123', 'D2V_124', 'D2V_125', 'D2V_126', 'D2V_127', 'D2V_128', 'D2V_129', 'D2V_130', 'D2V_131', 'D2V_132', 'D2V_133', 'D2V_134', 'D2V_135', 'D2V_136', 'D2V_137', 'D2V_138', 'D2V_139', 'D2V_140', 'D2V_141', 'D2V_142', 'D2V_143', 'D2V_144', 'D2V_145', 'D2V_146', 'D2V_147', 'D2V_148', 'D2V_149', 'D2V_150', 'D2V_151', 'D2V_152', 'D2V_153', 'D2V_154', 'D2V_155', 'D2V_156', 'D2V_157', 'D2V_158', 'D2V_159', 'D2V_160', 'D2V_161', 'D2V_162', 'D2V_163', 'D2V_164', 'D2V_165', 'D2V_166', 'D2V_167', 'D2V_168', 'D2V_169', 'D2V_170', 'D2V_171', 'D2V_172', 'D2V_173', 'D2V_174', 'D2V_175', 'D2V_176', 'D2V_177', 'D2V_178', 'D2V_179', 'D2V_180', 'D2V_181', 'D2V_182', 'D2V_183', 'D2V_184', 'D2V_185', 'D2V_186', 'D2V_187', 'D2V_188', 'D2V_189', 'D2V_190', 'D2V_191', 'D2V_192', 'D2V_193', 'D2V_194', 'D2V_195', 'D2V_196', 'D2V_197', 'D2V_198', 'D2V_199', 'D2V_200', 'D2V_201', 'D2V_202', 'D2V_203', 'D2V_204', 'D2V_205', 'D2V_206', 'D2V_207', 'D2V_208', 'D2V_209', 'D2V_210', 'D2V_211', 'D2V_212', 'D2V_213', 'D2V_214', 'D2V_215', 'D2V_216', 'D2V_217', 'D2V_218', 'D2V_219', 'D2V_220', 'D2V_221', 'D2V_222', 'D2V_223', 'D2V_224', 'D2V_225', 'D2V_226', 'D2V_227', 'D2V_228', 'D2V_229', 'D2V_230', 'D2V_231', 'D2V_232', 'D2V_233', 'D2V_234', 'D2V_235', 'D2V_236', 'D2V_237', 'D2V_238', 'D2V_239', 'D2V_240', 'D2V_241', 'D2V_242', 'D2V_243', 'D2V_244', 'D2V_245', 'D2V_246', 'D2V_247', 'D2V_248', 'D2V_249', 'D2V_250', 'D2V_251', 'D2V_252', 'D2V_253', 'D2V_254', 'D2V_255', 'D2V_256', 'D2V_257', 'D2V_258', 'D2V_259', 'D2V_260', 'D2V_261', 'D2V_262', 'D2V_263', 'D2V_264', 'D2V_265', 'D2V_266', 'D2V_267', 'D2V_268', 'D2V_269', 'D2V_270', 'D2V_271', 'D2V_272', 'D2V_273', 'D2V_274', 'D2V_275', 'D2V_276', 'D2V_277', 'D2V_278', 'D2V_279', 'D2V_280', 'D2V_281', 'D2V_282', 'D2V_283', 'D2V_284', 'D2V_285', 'D2V_286', 'D2V_287', 'D2V_288', 'D2V_289', 'D2V_290', 'D2V_291', 'D2V_292', 'D2V_293', 'D2V_294', 'D2V_295', 'D2V_296', 'D2V_297', 'D2V_298', 'D2V_299', 'A01', 'A02', 'B01', 'B02', 'C01', 'C02', 'C03', 'D01', 'D02', 'dadungi', 'Deoyoung', 'Robs', 'himart']\n"
     ]
    }
   ],
   "source": [
    "print(Continuous_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Continous_base =\\\n",
    "[ tf.contrib.layers.real_valued_column( continuous ) for continuous in Continuous_col ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segment Column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17']\n"
     ]
    }
   ],
   "source": [
    "print(Categorical_Seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical_base = []\n",
    "for i in Categorical_Seg :\n",
    "    key = Input[i].astype(str).unique().tolist()\n",
    "    if len(key) < 5 :\n",
    "        value = tf.contrib.layers.sparse_column_with_keys(i , keys = key)\n",
    "        Categorical_base.append(value)\n",
    "    else :\n",
    "        bucket_size = 1000 if len(key) > 10 else 100\n",
    "#         value = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "#             key= i ,vocabulary_list= key )\n",
    "        value = tf.contrib.layers.sparse_column_with_hash_bucket(i , hash_bucket_size= bucket_size  )\n",
    "        Categorical_base.append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Seg = Categorical_Seg\n",
    "Seg_dict = dict(zip(Seg , list(np.arange(len(Seg)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_col = [[Seg[i], Seg[i + j + 1]]\n",
    "        for i in range(16) for j in range(16-i)]\n",
    "Cross_Columns = []\n",
    "for i , j in cross_col :\n",
    "    id1 , id2 = Seg_dict[i] , Seg_dict[j]\n",
    "    cross_tf = tf.contrib.layers.crossed_column\\\n",
    "    ([Categorical_base[id1], Categorical_base[id2]],\n",
    "     hash_bucket_size=int(1e4))\n",
    "    Cross_Columns.append(cross_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['X1', 'X2'], ['X1', 'X3'], ['X1', 'X4'], ['X1', 'X5'], ['X1', 'X6'], ['X1', 'X7'], ['X1', 'X8'], ['X1', 'X9'], ['X1', 'X10'], ['X1', 'X11'], ['X1', 'X12'], ['X1', 'X13'], ['X1', 'X14'], ['X1', 'X15'], ['X1', 'X16'], ['X1', 'X17'], ['X2', 'X3'], ['X2', 'X4'], ['X2', 'X5'], ['X2', 'X6'], ['X2', 'X7'], ['X2', 'X8'], ['X2', 'X9'], ['X2', 'X10'], ['X2', 'X11'], ['X2', 'X12'], ['X2', 'X13'], ['X2', 'X14'], ['X2', 'X15'], ['X2', 'X16'], ['X2', 'X17'], ['X3', 'X4'], ['X3', 'X5'], ['X3', 'X6'], ['X3', 'X7'], ['X3', 'X8'], ['X3', 'X9'], ['X3', 'X10'], ['X3', 'X11'], ['X3', 'X12'], ['X3', 'X13'], ['X3', 'X14'], ['X3', 'X15'], ['X3', 'X16'], ['X3', 'X17'], ['X4', 'X5'], ['X4', 'X6'], ['X4', 'X7'], ['X4', 'X8'], ['X4', 'X9'], ['X4', 'X10'], ['X4', 'X11'], ['X4', 'X12'], ['X4', 'X13'], ['X4', 'X14'], ['X4', 'X15'], ['X4', 'X16'], ['X4', 'X17'], ['X5', 'X6'], ['X5', 'X7'], ['X5', 'X8'], ['X5', 'X9'], ['X5', 'X10'], ['X5', 'X11'], ['X5', 'X12'], ['X5', 'X13'], ['X5', 'X14'], ['X5', 'X15'], ['X5', 'X16'], ['X5', 'X17'], ['X6', 'X7'], ['X6', 'X8'], ['X6', 'X9'], ['X6', 'X10'], ['X6', 'X11'], ['X6', 'X12'], ['X6', 'X13'], ['X6', 'X14'], ['X6', 'X15'], ['X6', 'X16'], ['X6', 'X17'], ['X7', 'X8'], ['X7', 'X9'], ['X7', 'X10'], ['X7', 'X11'], ['X7', 'X12'], ['X7', 'X13'], ['X7', 'X14'], ['X7', 'X15'], ['X7', 'X16'], ['X7', 'X17'], ['X8', 'X9'], ['X8', 'X10'], ['X8', 'X11'], ['X8', 'X12'], ['X8', 'X13'], ['X8', 'X14'], ['X8', 'X15'], ['X8', 'X16'], ['X8', 'X17'], ['X9', 'X10'], ['X9', 'X11'], ['X9', 'X12'], ['X9', 'X13'], ['X9', 'X14'], ['X9', 'X15'], ['X9', 'X16'], ['X9', 'X17'], ['X10', 'X11'], ['X10', 'X12'], ['X10', 'X13'], ['X10', 'X14'], ['X10', 'X15'], ['X10', 'X16'], ['X10', 'X17'], ['X11', 'X12'], ['X11', 'X13'], ['X11', 'X14'], ['X11', 'X15'], ['X11', 'X16'], ['X11', 'X17'], ['X12', 'X13'], ['X12', 'X14'], ['X12', 'X15'], ['X12', 'X16'], ['X12', 'X17'], ['X13', 'X14'], ['X13', 'X15'], ['X13', 'X16'], ['X13', 'X17'], ['X14', 'X15'], ['X14', 'X16'], ['X14', 'X17'], ['X15', 'X16'], ['X15', 'X17'], ['X16', 'X17']]\n"
     ]
    }
   ],
   "source": [
    "print(cross_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Column\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"./images/Categorical_Embedding.PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Gender', 'AgeGroup', 'A_residential_area']\n"
     ]
    }
   ],
   "source": [
    "print(Categorical_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Categorical_base2 =\\\n",
    "[ tf.contrib.layers.sparse_column_with_keys( Cat , keys = Input[Cat].unique().tolist() ) for Cat in Categorical_col ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide Columns\n",
    "---\n",
    "\n",
    "Categorical Column + Segment Column + Cross Column + Doc2Vec\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_columns  = Categorical_base + Categorical_base2 + Cross_Columns + Continous_base[0:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#https://tensorflowkorea.gitbooks.io/tensorflow-kr/content/g3doc/tutorials/wide_and_deep/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Column\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n",
      "WARNING:tensorflow:The default stddev value of initializer was changed from \"1/sqrt(vocab_size)\" to \"1/sqrt(dimension)\" in core implementation (tf.feature_column.embedding_column).\n"
     ]
    }
   ],
   "source": [
    "embed_col = []\n",
    "for idx , i in enumerate(Categorical_col) :\n",
    "    key = Categorical_base2[idx]\n",
    "    k = 8\n",
    "    dim = int( np.ceil( np.sqrt(Input[i].nunique()) ) ) * k\n",
    "    emb = tf.contrib.layers.embedding_column(key , dimension=dim)\n",
    "    embed_col.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[_EmbeddingColumn(sparse_id_column=_SparseColumnKeys(column_name='Gender', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('F',), num_oov_buckets=0, vocab_size=1, default_value=-1), combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f442ee760f0>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumnKeys(column_name='AgeGroup', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('50세~54세',), num_oov_buckets=0, vocab_size=1, default_value=-1), combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f442ee76c18>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True),\n",
       " _EmbeddingColumn(sparse_id_column=_SparseColumnKeys(column_name='A_residential_area', is_integerized=False, bucket_size=None, lookup_config=_SparseIdLookupConfig(vocabulary_file=None, keys=('A24',), num_oov_buckets=0, vocab_size=1, default_value=-1), combiner='sum', dtype=tf.string), dimension=8, combiner='mean', initializer=<tensorflow.python.ops.init_ops.TruncatedNormal object at 0x7f442ee76c50>, ckpt_to_load_from=None, tensor_name_in_ckpt=None, shared_embedding_name=None, shared_vocab_size=None, max_norm=None, trainable=True)]"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Columns\n",
    "---\n",
    "\n",
    "Embedding Column + Continuous Column\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_columns = embed_col + Continous_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/var/disk/WideDeep_ver2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      "intra_op_parallelism_threads: 8\n",
      "inter_op_parallelism_threads: 8\n",
      "gpu_options {\n",
      "  allow_growth: true\n",
      "}\n",
      "log_device_placement: true\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f4036b21048>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "inter_op = 8\n",
    "intra_op = 8\n",
    "config = tf.ConfigProto(device_count={'GPU': 0},\n",
    "                        log_device_placement=True , \n",
    "                        inter_op_parallelism_threads=inter_op,\n",
    "                        intra_op_parallelism_threads=intra_op)\n",
    "config.gpu_options.allow_growth = True\n",
    "run_config = tf.estimator.RunConfig().replace(\n",
    "    session_config=config)\n",
    "model_dir = \"/var/disk/WideDeep_ver2\"\n",
    "m = tf.estimator.DNNLinearCombinedClassifier(\n",
    "    model_dir=model_dir,\n",
    "    linear_feature_columns=wide_columns,\n",
    "    dnn_feature_columns=deep_columns,\n",
    "    dnn_activation_fn=tf.nn.leaky_relu ,\n",
    "    n_classes = test_label_eval.shape[1] , \n",
    "    batch_norm = True,\n",
    "    dnn_dropout = 0.3,\n",
    "    dnn_hidden_units=[200 ,  150 , 100 , 50],\n",
    "    dnn_optimizer=tf.train.AdamOptimizer(learning_rate = 0.01) , \n",
    "    linear_optimizer=tf.train.AdamOptimizer(learning_rate = 0.01),\n",
    "    config=run_config\n",
    ") \n",
    "#     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.decode_csv 버전 \n",
    "## Error 발생 해결1[URL](https://stackoverflow.com/questions/39730528/typeerror-signature-mismatch-keys-must-be-dtype-dtype-string-got-dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "_CSV_COLUMNS = Input.columns.tolist()\n",
    "_CSV_COLUMN_DEFAULTS = [[0.0]] * len(Continuous_col1) +\\\n",
    "[[\"\"]] * len(Categorical_col) +\\\n",
    "[[0.0]] * len(Continuous_col3 + Continuous_col2) +\\\n",
    "[[\"\"]] * len(Categorical_Seg) +\\\n",
    "[[0]] * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(_CSV_COLUMNS) == len(_CSV_COLUMN_DEFAULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns = Input.columns.tolist()\n",
    "CONTINUOUS_COLUMNS = Continuous_col\n",
    "CATEGORICAL_COLUMNS = Categorical_col +  Categorical_Seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical : \n",
      " ['Gender', 'AgeGroup', 'A_residential_area', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17']\n",
      " \n",
      "Continuous : \n",
      " ['D2V_0', 'D2V_1', 'D2V_2', 'D2V_3', 'D2V_4', 'D2V_5', 'D2V_6', 'D2V_7', 'D2V_8', 'D2V_9', 'D2V_10', 'D2V_11', 'D2V_12', 'D2V_13', 'D2V_14', 'D2V_15', 'D2V_16', 'D2V_17', 'D2V_18', 'D2V_19', 'D2V_20', 'D2V_21', 'D2V_22', 'D2V_23', 'D2V_24', 'D2V_25', 'D2V_26', 'D2V_27', 'D2V_28', 'D2V_29', 'D2V_30', 'D2V_31', 'D2V_32', 'D2V_33', 'D2V_34', 'D2V_35', 'D2V_36', 'D2V_37', 'D2V_38', 'D2V_39', 'D2V_40', 'D2V_41', 'D2V_42', 'D2V_43', 'D2V_44', 'D2V_45', 'D2V_46', 'D2V_47', 'D2V_48', 'D2V_49', 'D2V_50', 'D2V_51', 'D2V_52', 'D2V_53', 'D2V_54', 'D2V_55', 'D2V_56', 'D2V_57', 'D2V_58', 'D2V_59', 'D2V_60', 'D2V_61', 'D2V_62', 'D2V_63', 'D2V_64', 'D2V_65', 'D2V_66', 'D2V_67', 'D2V_68', 'D2V_69', 'D2V_70', 'D2V_71', 'D2V_72', 'D2V_73', 'D2V_74', 'D2V_75', 'D2V_76', 'D2V_77', 'D2V_78', 'D2V_79', 'D2V_80', 'D2V_81', 'D2V_82', 'D2V_83', 'D2V_84', 'D2V_85', 'D2V_86', 'D2V_87', 'D2V_88', 'D2V_89', 'D2V_90', 'D2V_91', 'D2V_92', 'D2V_93', 'D2V_94', 'D2V_95', 'D2V_96', 'D2V_97', 'D2V_98', 'D2V_99', 'D2V_100', 'D2V_101', 'D2V_102', 'D2V_103', 'D2V_104', 'D2V_105', 'D2V_106', 'D2V_107', 'D2V_108', 'D2V_109', 'D2V_110', 'D2V_111', 'D2V_112', 'D2V_113', 'D2V_114', 'D2V_115', 'D2V_116', 'D2V_117', 'D2V_118', 'D2V_119', 'D2V_120', 'D2V_121', 'D2V_122', 'D2V_123', 'D2V_124', 'D2V_125', 'D2V_126', 'D2V_127', 'D2V_128', 'D2V_129', 'D2V_130', 'D2V_131', 'D2V_132', 'D2V_133', 'D2V_134', 'D2V_135', 'D2V_136', 'D2V_137', 'D2V_138', 'D2V_139', 'D2V_140', 'D2V_141', 'D2V_142', 'D2V_143', 'D2V_144', 'D2V_145', 'D2V_146', 'D2V_147', 'D2V_148', 'D2V_149', 'D2V_150', 'D2V_151', 'D2V_152', 'D2V_153', 'D2V_154', 'D2V_155', 'D2V_156', 'D2V_157', 'D2V_158', 'D2V_159', 'D2V_160', 'D2V_161', 'D2V_162', 'D2V_163', 'D2V_164', 'D2V_165', 'D2V_166', 'D2V_167', 'D2V_168', 'D2V_169', 'D2V_170', 'D2V_171', 'D2V_172', 'D2V_173', 'D2V_174', 'D2V_175', 'D2V_176', 'D2V_177', 'D2V_178', 'D2V_179', 'D2V_180', 'D2V_181', 'D2V_182', 'D2V_183', 'D2V_184', 'D2V_185', 'D2V_186', 'D2V_187', 'D2V_188', 'D2V_189', 'D2V_190', 'D2V_191', 'D2V_192', 'D2V_193', 'D2V_194', 'D2V_195', 'D2V_196', 'D2V_197', 'D2V_198', 'D2V_199', 'D2V_200', 'D2V_201', 'D2V_202', 'D2V_203', 'D2V_204', 'D2V_205', 'D2V_206', 'D2V_207', 'D2V_208', 'D2V_209', 'D2V_210', 'D2V_211', 'D2V_212', 'D2V_213', 'D2V_214', 'D2V_215', 'D2V_216', 'D2V_217', 'D2V_218', 'D2V_219', 'D2V_220', 'D2V_221', 'D2V_222', 'D2V_223', 'D2V_224', 'D2V_225', 'D2V_226', 'D2V_227', 'D2V_228', 'D2V_229', 'D2V_230', 'D2V_231', 'D2V_232', 'D2V_233', 'D2V_234', 'D2V_235', 'D2V_236', 'D2V_237', 'D2V_238', 'D2V_239', 'D2V_240', 'D2V_241', 'D2V_242', 'D2V_243', 'D2V_244', 'D2V_245', 'D2V_246', 'D2V_247', 'D2V_248', 'D2V_249', 'D2V_250', 'D2V_251', 'D2V_252', 'D2V_253', 'D2V_254', 'D2V_255', 'D2V_256', 'D2V_257', 'D2V_258', 'D2V_259', 'D2V_260', 'D2V_261', 'D2V_262', 'D2V_263', 'D2V_264', 'D2V_265', 'D2V_266', 'D2V_267', 'D2V_268', 'D2V_269', 'D2V_270', 'D2V_271', 'D2V_272', 'D2V_273', 'D2V_274', 'D2V_275', 'D2V_276', 'D2V_277', 'D2V_278', 'D2V_279', 'D2V_280', 'D2V_281', 'D2V_282', 'D2V_283', 'D2V_284', 'D2V_285', 'D2V_286', 'D2V_287', 'D2V_288', 'D2V_289', 'D2V_290', 'D2V_291', 'D2V_292', 'D2V_293', 'D2V_294', 'D2V_295', 'D2V_296', 'D2V_297', 'D2V_298', 'D2V_299', 'A01', 'A02', 'B01', 'B02', 'C01', 'C02', 'C03', 'D01', 'D02', 'dadungi', 'Deoyoung', 'Robs', 'himart']\n"
     ]
    }
   ],
   "source": [
    "print(\"Categorical : \\n\" ,  CATEGORICAL_COLUMNS)\n",
    "print(\" \")\n",
    "print(\"Continuous : \\n\" ,  CONTINUOUS_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(data_file, num_epochs, shuffle, batch_size , Eval ):\n",
    "    def parse_csv(data_file ):\n",
    "        print('Parsing', data_file)\n",
    "        print(Eval)\n",
    "        if Eval == \"train\" :\n",
    "            columns = tf.decode_csv(data_file , record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "            features = dict(zip(_CSV_COLUMNS, columns))\n",
    "            labels = features.pop('target')\n",
    "        elif Eval == \"test\" :\n",
    "            columns = tf.decode_csv(data_file , record_defaults=_CSV_COLUMN_DEFAULTS[:-1])\n",
    "            features = dict(zip(_CSV_COLUMNS[:-1] , columns))\n",
    "            labels = tf.constant([1] , dtype = tf.int64 , shape = () ) # , name =\"target\" \n",
    "        return features, labels\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_file).skip(1)\n",
    "    if shuffle :\n",
    "        dataset = dataset.shuffle(buffer_size=10000) \n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=10)\n",
    "    if Eval == \"train\" :\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        dataset = dataset.batch(batch_size , drop_remainder= True)\n",
    "    else :\n",
    "        dataset = dataset.batch(batch_size , drop_remainder= True)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels\n",
    "\n",
    "def train_input_fn():\n",
    "    return input_fn(data_file = \"./Total_Data_ver2.csv\" ,\n",
    "                    num_epochs= 10 ,\n",
    "                    shuffle= True ,\n",
    "                    batch_size= 256,\n",
    "                    Eval = \"train\"\n",
    "                   )\n",
    "\n",
    "def csv_input(data):\n",
    "    continuous_cols = {k: tf.constant(data[k].values)\n",
    "                       for k in CONTINUOUS_COLUMNS}\n",
    "    categorical_cols = {k: tf.SparseTensor( indices=[[i, 0] for i in range(data[k].size)], values=data[k].astype(str).values , \n",
    "                                           dense_shape=[data[k].size, 1] ) for k in CATEGORICAL_COLUMNS}\n",
    "\n",
    "    feature_cols = {**continuous_cols , **categorical_cols}\n",
    "    labels = None\n",
    "    return feature_cols , labels\n",
    "\n",
    "def csv_input_fn():\n",
    "    csv = csv_input(data = test_)    \n",
    "    return csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "train\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:loss = 1748.514, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 75 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.124863\n",
      "INFO:tensorflow:loss = 1284.9064, step = 101 (800.881 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 155 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.136656\n",
      "INFO:tensorflow:loss = 1308.0979, step = 201 (731.760 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 236 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.123011\n",
      "INFO:tensorflow:loss = 1358.6477, step = 301 (812.935 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 305 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 372 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:966: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:global_step/sec: 0.101943\n",
      "INFO:tensorflow:loss = 1180.5647, step = 401 (980.941 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 436 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.121338\n",
      "INFO:tensorflow:loss = 1206.186, step = 501 (824.145 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 506 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 571 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.101743\n",
      "INFO:tensorflow:loss = 1282.7599, step = 601 (982.873 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 641 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.12285\n",
      "INFO:tensorflow:loss = 1181.1829, step = 701 (813.998 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 708 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 773 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.101991\n",
      "INFO:tensorflow:loss = 1187.3425, step = 801 (980.479 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 841 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.125704\n",
      "INFO:tensorflow:loss = 1165.3745, step = 901 (795.522 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 913 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 982 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.104059\n",
      "INFO:tensorflow:loss = 1194.2589, step = 1001 (960.997 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1050 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.121277\n",
      "INFO:tensorflow:loss = 1222.7427, step = 1101 (824.561 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1116 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1187 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.104417\n",
      "INFO:tensorflow:loss = 1309.7964, step = 1201 (957.696 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1256 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.123496\n",
      "INFO:tensorflow:loss = 1309.4171, step = 1301 (809.744 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1323 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1387 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0989665\n",
      "INFO:tensorflow:loss = 1198.338, step = 1401 (1010.443 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1453 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.123476\n",
      "INFO:tensorflow:loss = 1295.2372, step = 1501 (809.875 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1522 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1587 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0999465\n",
      "INFO:tensorflow:loss = 1127.8945, step = 1601 (1000.536 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1653 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.12048\n",
      "INFO:tensorflow:loss = 1327.8015, step = 1701 (830.011 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1719 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1784 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.102155\n",
      "INFO:tensorflow:loss = 1161.9492, step = 1801 (978.902 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1855 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.120395\n",
      "INFO:tensorflow:loss = 1121.4661, step = 1901 (830.597 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1919 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 1980 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0968732\n",
      "INFO:tensorflow:loss = 1163.8081, step = 2001 (1032.277 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2045 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.122629\n",
      "INFO:tensorflow:loss = 1146.8582, step = 2101 (815.468 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2113 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2178 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0989162\n",
      "INFO:tensorflow:loss = 1159.2506, step = 2201 (1010.957 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2244 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.123384\n",
      "INFO:tensorflow:loss = 1091.9731, step = 2301 (810.475 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2312 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2377 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.100835\n",
      "INFO:tensorflow:loss = 1144.0652, step = 2401 (991.722 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2445 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.124752\n",
      "INFO:tensorflow:loss = 1142.3846, step = 2501 (801.593 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2515 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2584 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.107151\n",
      "INFO:tensorflow:loss = 1237.3843, step = 2601 (933.263 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2653 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.12336\n",
      "INFO:tensorflow:loss = 1242.75, step = 2701 (810.638 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2723 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 2795 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.107763\n",
      "INFO:tensorflow:loss = 1228.2703, step = 2801 (927.962 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2864 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.124074\n",
      "INFO:tensorflow:loss = 1163.009, step = 2901 (805.970 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2936 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.12836\n",
      "INFO:tensorflow:loss = 1153.4897, step = 3001 (779.060 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3010 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 3082 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.10874\n",
      "INFO:tensorflow:loss = 1143.5537, step = 3101 (919.626 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3151 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.125195\n",
      "INFO:tensorflow:loss = 1136.5776, step = 3201 (798.754 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3224 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 3296 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.107915\n",
      "INFO:tensorflow:loss = 1145.2454, step = 3301 (926.654 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3364 into /var/disk/WideDeep_ver2/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.122883\n",
      "INFO:tensorflow:loss = 1138.9651, step = 3401 (813.784 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3434 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.126288\n",
      "INFO:tensorflow:loss = 1126.6016, step = 3501 (791.841 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3506 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 3579 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.110636\n",
      "INFO:tensorflow:loss = 1135.1769, step = 3601 (903.864 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3651 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.129213\n",
      "INFO:tensorflow:loss = 1176.3572, step = 3701 (773.918 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3726 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 3799 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.111138\n",
      "INFO:tensorflow:loss = 1102.6376, step = 3801 (899.780 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3873 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.131169\n",
      "INFO:tensorflow:loss = 1186.151, step = 3901 (762.372 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3947 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.130847\n",
      "INFO:tensorflow:loss = 1257.6527, step = 4001 (764.253 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4023 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 4093 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.106405\n",
      "INFO:tensorflow:loss = 1299.8462, step = 4101 (939.807 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4162 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.122311\n",
      "INFO:tensorflow:loss = 1275.4906, step = 4201 (817.587 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4230 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.126425\n",
      "INFO:tensorflow:loss = 1191.0598, step = 4301 (790.984 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4304 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 4373 into /var/disk/WideDeep_ver2/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.106708\n",
      "INFO:tensorflow:loss = 1245.729, step = 4401 (937.136 sec)\n"
     ]
    }
   ],
   "source": [
    "m.train(input_fn=train_input_fn , \n",
    "        max_steps= 10000 ,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Parsing Tensor(\"arg0:0\", shape=(), dtype=string, device=/device:CPU:0)\n",
      "train\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:1901: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/lookup_ops.py:1137: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/layers/python/layers/feature_column.py:2390: calling sparse_feature_cross (from tensorflow.contrib.layers.python.ops.sparse_feature_cross_op) with hash_key=None is deprecated and will be removed after 2016-11-20.\n",
      "Instructions for updating:\n",
      "The default behavior of sparse_feature_cross is changing, the default\n",
      "value for hash_key will change to SPARSE_FEATURE_CROSS_DEFAULT_HASH_KEY.\n",
      "From that point on sparse_feature_cross will always use FingerprintCat64\n",
      "to concatenate the feature fingerprints. And the underlying\n",
      "_sparse_feature_cross_op.sparse_feature_cross operation will be marked\n",
      "as deprecated.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-07-21T01:49:23Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /root/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /var/disk/WideDeep/model.ckpt-5411\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2019-07-21-01:49:40\n",
      "INFO:tensorflow:Saving dict for global step 5411: accuracy = 0.04296875, average_loss = 4.82559, global_step = 5411, loss = 1235.3511\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5411: /var/disk/WideDeep/model.ckpt-5411\n",
      "accuracy: 0.04296875\n",
      "average_loss: 4.82559\n",
      "global_step: 5411\n",
      "loss: 1235.3511\n"
     ]
    }
   ],
   "source": [
    "results = m.evaluate(input_fn=train_input_fn , steps=1)\n",
    "for key in sorted(results):\n",
    "    print(\"%s: %s\" % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST CSV Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Predict 해결법](https://stackoverflow.com/questions/46857382/tensorflow-estimator-predict-gives-warningtensorflowinput-graph-does-not-con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Input graph does not use tf.data.Dataset or contain a QueueRunner. That means predict yields forever. This is probably a mistake.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/disk/WideDeep/model.ckpt-5480\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Start\n",
      "eval time :  213.02658796310425\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "csvProb = m.predict(input_fn= csv_input_fn , predict_keys=\"probabilities\" )\n",
    "for i in np.arange( test_.shape[0] ) :   \n",
    "    prob_next = csvProb.__next__()[\"probabilities\"].reshape(1, -1 )\n",
    "    if i == 0 :\n",
    "        print('Start')\n",
    "        prob = prob_next\n",
    "    else :\n",
    "        prob = np.concatenate((prob,prob_next),axis = 0)\n",
    "print(\"eval time : \" , time.time() - start)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13915, 884)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(score_matrix, top_n, test_matix):\n",
    "    avg_acc = 0\n",
    "    for i in range(len(score_matrix)):\n",
    "        top = score_matrix.iloc[i].nlargest(top_n).index\n",
    "        tmp = 0\n",
    "        for j in range(len(top)):\n",
    "            true_buy = [i.split(\"/\")[0] for i in test_matix[\"item\"][0].split()]\n",
    "            if top[j] in true_buy :\n",
    "                tmp += 1\n",
    "\n",
    "        acc = tmp / min(len(top) , len(true_buy))\n",
    "        avg_acc += acc / len(score_matrix)\n",
    "\n",
    "    return print(\"Hit rate of Top {}: {:>.5f}\".format(top_n, avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 몇개 이상 구매한 사람 기준으로 추천해줬을 때\n",
    "\n",
    "---\n",
    "\n",
    "### $$\\frac{추천한\\ 것\\ 중에서\\ 구매를\\ 한\\ 수 }{min(추천수 , 구매수) \\quad \\quad }$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pd = pd.DataFrame(prob, columns=test_label_eval.columns)\n",
    "cond = list(np.sum(test_label_eval.values , axis = 1) > 10 )\n",
    "cond_pd = score_pd[cond].reset_index(drop= True)\n",
    "test_pd = test_label[cond].reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate of Top 10: 0.12169\n"
     ]
    }
   ],
   "source": [
    "get_acc(cond_pd, 10 , test_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_idx(prob, N , test_label_eval , idx , dict_store ) :\n",
    "    def top_n(data , n ) :\n",
    "        return list(map(lambda x: np.argpartition(x, -n)[-n:][0], data ))[0]\n",
    "    true_label =\\\n",
    "    np.where(test_label_eval.iloc[idx : (idx+1),:].values == 1 )[1].tolist()    \n",
    "    recommend = [top_n(prob , i) for i in np.arange(1,N+1)]\n",
    "    true_label = np.where(test_label_eval.iloc[idx : (idx+1),:].values == 1 )[1].tolist()\n",
    "    target = test_label_eval.columns.tolist()\n",
    "    REC = [target[i] for i in recommend]\n",
    "    REC = [dict_store[i] for i in  REC ]\n",
    "    correct = 0\n",
    "    for rec in recommend :\n",
    "        if rec in true_label :\n",
    "            correct += 1 \n",
    "    acc = correct / len(recommend) \n",
    "    print(\"Hit rate of Top {}: {:>.5f}\".format(N , acc))\n",
    "    return acc , REC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15개를 구매한 유저 중에서 한 유저에 대한 분석\n",
    "\n",
    "> 10개를 추천해줬을 때 얼마나 실제 구매한 것이 포함되어 있는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for i in test_label[\"item\"].values.tolist() :\n",
    "    store = i.split()\n",
    "    for j in store : \n",
    "        list_.append(j)\n",
    "Store = np.unique(list_)\n",
    "dict_store = {}\n",
    "for st in Store :\n",
    "    dict_store[st.split(\"/\")[0]] = st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate of Top 9: 0.11111\n",
      "\n",
      " 추천 한 물품 : \n",
      " C150302/쿠키 | C010110/딸기 | C170701/생활잡화균일가 | C150506/젤리 | C150405/일반스낵 | C010206/바나나 | C150306/파이 | C150103/국물용기라면 | C170206/종량제봉투\n"
     ]
    }
   ],
   "source": [
    "cond = list(np.sum(test_label_eval.values , axis = 1) == 3  )\n",
    "user_pd = test_label[cond].reset_index(drop= True)\n",
    "cond_pd = prob[cond]\n",
    "test_eval_pd = test_label_eval[cond].reset_index(drop= True)\n",
    "idx = 200\n",
    "ACC , REC = get_acc_idx(cond_pd , 9 , test_eval_pd , idx , dict_store )\n",
    "\n",
    "print(\"\\n 추천 한 물품 : \\n\" , \" | \".join(REC) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID : 10951\n",
    "\n",
    "---\n",
    "\n",
    "* 구매했던 상품들\n",
    "\n",
    "상품 명             | 상품 명          | 상품 명\n",
    ":-------------------|:-----------------|:------------------\n",
    "C130204/스틱원두커피 |C170414/여성타이즈 | C170701/생활잡화균일가\n",
    "\n",
    "<br>\n",
    "\n",
    "* 추천 상품 \n",
    "\n",
    "상품 명             | 상품 명          | 상품 명\n",
    ":-------------------|:-----------------|:------------------\n",
    " C150302/쿠키 | C010110/딸기 | C170701/생활잡화균일가 \n",
    " C150506/젤리 | C150405/일반스낵 | C010206/바나나 \n",
    " C150306/파이 | C150103/국물용기라면 | C170206/종량제봉투\n",
    " \n",
    " <br>\n",
    " \n",
    "* **Hit rate of Top 9: 0.11111**\n",
    "\n",
    "### $\\frac{추천해준\\ 상품에서\\ 실제\\ 구매한\\ 상품의\\ 개수}{min(추천해준\\ 상품의\\ 개수 , 구매수) \\quad \\quad \\quad }$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit rate of Top 9: 0.11111\n",
      "\n",
      " 추천 한 물품 : \n",
      " C060406/베이커리일반빵 | C060108/즉석반찬원부재료 | C060206/온장조리원부재료 | C060504/기타냉장조리 | C060402/일반빵 | C060205/즉석어묵 | C060506/일반떡 | C060403/호빵찐빵 | C060501/한과\n",
      " \n",
      "item     C060402/일반빵\n",
      "영수증번호         173825\n",
      "고객번호           10198\n",
      "Name: 25, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cond = list(np.sum(test_label_eval.values , axis = 1) == 1  )\n",
    "user_pd = test_label[cond].reset_index(drop= True)\n",
    "cond_pd = prob[cond]\n",
    "test_eval_pd = test_label_eval[cond].reset_index(drop= True)\n",
    "idx = 25\n",
    "ACC , REC = get_acc_idx(cond_pd , 9 , test_eval_pd , idx , dict_store )\n",
    "\n",
    "print(\"\\n 추천 한 물품 : \\n\" , \" | \".join(REC) )\n",
    "print(\" \")\n",
    "print(user_pd.iloc[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID : 10198\n",
    "\n",
    "---\n",
    "\n",
    "* 구매했던 상품들\n",
    "\n",
    "상품 명             | \n",
    ":-------------------|\n",
    "C060402/일반빵 |\n",
    "\n",
    "<br>\n",
    "\n",
    "* 추천 상품 \n",
    "\n",
    "상품 명             | 상품 명          | 상품 명\n",
    ":-------------------|:-----------------|:------------------\n",
    " C060406/베이커리일반빵 | C060108/즉석반찬원부재료 | C060206/온장조리원부재료 \n",
    " C060504/기타냉장조리 | C060402/일반빵 | C060205/즉석어묵 \n",
    " C060506/일반떡 | C060403/호빵찐빵 | C060501/한과\n",
    " \n",
    " <br>\n",
    " \n",
    "* **Hit rate of Top 9: 0.11111**\n",
    "\n",
    "### $\\frac{추천해준\\ 상품에서\\ 실제\\ 구매한\\ 상품의\\ 개수}{min(추천해준\\ 상품의\\ 개수 , 구매수) \\quad \\quad \\quad }$\n",
    "\n",
    "> 10198번 고객은 __'일반빵'__을 1번 구매한 고객입니다.\n",
    "이런 경우에 알고리즘에서는 __'베이커리일반빵' , '일반빵','일반떡'__등 상위카테고리 06 범주에서 비슷한 상품들을 추천해주는 모습입니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
