{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Url](https://imbalanced-learn.readthedocs.io/en/stable/combine.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "* marginal outlier 와 inliner 사이에 새로운 데이터 포인트를 생성시킨다. \n",
    "* 이것은 oversampling 으로 부터 발생하는 공간을 지울 수 있으므로 해결할 수 있다?\n",
    "* The two ready-to use classes imbalanced-learn implements for combining over- and undersampling methods are: (i) `SMOTETomek` [BPM2004] and (ii) `SMOTEENN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,\n",
    "                            n_redundant=0, n_repeated=0, n_classes=3,\n",
    "                            n_clusters_per_class=1,\n",
    "                            weights=[0.01, 0.05, 0.94],\n",
    "                            class_sep=0.8, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 64), (1, 262), (2, 4674)]\n"
     ]
    }
   ],
   "source": [
    "print( sorted( Counter(y).items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 4060), (1, 4381), (2, 3502)]\n"
     ]
    }
   ],
   "source": [
    "smote_enn = SMOTEENN( random_state= 0 )\n",
    "X_resampled , y_resampled = smote_enn.fit_resample(X , y)\n",
    "\n",
    "print(sorted(Counter(y_resampled).items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble of samplers \n",
    "\n",
    "### 5.1 Classifier including inner balancing samplers \n",
    "\n",
    "#### 5.1.1 Bagging Classifier\n",
    "\n",
    "* `BaggingClassifer` 가 있는데, 그러나 이것은 데이터의 균형을 맞춰주지 못하고 대다수 class에 더 선호한다.\n",
    "\n",
    "`balanced_accuract_score`\n",
    "\n",
    "* `BalancedBaggingClassifier` 은 각각의 subset에서 데이터를 학습전에 Resample 한다. \n",
    "\n",
    "* `Bagging` 이란 원래 데이터를 여러개의 작은 N개를 샘플링해서 만든 다음 각각의 작은 데이터로 모델링을 하고 이것을 합쳐서 최종적인 모델로 사용한다는 개념\n",
    "\n",
    "* `Boosting` 은 간단한 모델로 전체를 학습시킨 다음, 간단한 모델이다 보니 에러가 남을텐데, 거기 중에서 에러가 큰 부분에 대해서 다시 모아서 다시 새로운 간단한 모델을 학습시키는 것을 반복한다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8864390007810891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "bc = BaggingClassifier(base_estimator = DecisionTreeClassifier(), random_state = 0)\n",
    "\n",
    "bc.fit(X_train , y_train)\n",
    "\n",
    "y_pred = bc.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8691879549364346"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "bbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                             sampling_strategy='auto',\n",
    "                             replacement=False,\n",
    "                             random_state=0)\n",
    "bbc.fit(X_train , y_train)\n",
    "\n",
    "y_pred = bbc.predict(X_test)\n",
    "balanced_accuracy_score(y_test , y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2  Forest of Randomized trees \n",
    "\n",
    "* `BalancedRandomForestClassifier` 은 각각의 트리에서 Balanced bootstrap sample 해서 생성되는 또다른 앙상블 방법이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8779524446169549"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "brf = BalancedRandomForestClassifier(n_estimators=100, random_state=0)\n",
    "brf.fit(X_train, y_train) \n",
    "y_pred = brf.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5976795, 0.4023205])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brf.feature_importances_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.3 Boosting \n",
    "\n",
    "`RUSBoostClassifier` 은 boosting iteration을 하기 전에 데이터를 랜덤하게 undersample 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134910366440966"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "rusboost = RUSBoostClassifier(random_state=0)\n",
    "rusboost.fit(X_train, y_train)  \n",
    "y_pred = rusboost.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AdaBoost` 은 bagging classifier 로 사용되고 `EasyEnsemble` 로 불린다.\n",
    "`EasyEnsembleClassifie` 는 adaboost 방법을 사용한다. \n",
    "*  BalancedBaggingClassifier 와 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7931491784189416"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.ensemble import EasyEnsembleClassifier\n",
    "eec = EasyEnsembleClassifier(random_state=0)\n",
    "eec.fit(X_train, y_train) \n",
    "y_pred = eec.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
