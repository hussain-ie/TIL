{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Medium SITE](https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "는 싫고 한글은 하다  쵝오 ㅋㅑㅋㅑ \n",
      "['韓子', ',', 'nice', '.', 'English', '-_-', './?!']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def test():\n",
    "    s='韓子는 싫고, 한글은 nice하다. English 쵝오 -_-ㅋㅑㅋㅑ ./?!'\n",
    "    hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기를 제외한 모든 글자\n",
    "  # hangul = re.compile('[^ \\u3131-\\u3163\\uac00-\\ud7a3]+')  # 위와 동일\n",
    "    result = hangul.sub('', s) # 한글과 띄어쓰기를 제외한 모든 부분을 제거\n",
    "    print (result)\n",
    "    result = hangul.findall(s) # 정규식에 일치되는 부분을 리스트 형태로 저장\n",
    "    print(result)\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예: 문자, 숫자가 아닌 데이터를 찾아서, '' 로 대체해라(삭제해라)\n",
    "\n",
    "* [^A-Za-z0-9]\t\\W\t문자, 숫자가 아닌 것을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(사람)11'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "string = \"(사람)11\"\n",
    "re.sub('[^A-Za-z0-9가-힣(*)]', '', string)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예: 한글만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'사람'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"1-----(사람)!@   1\"\n",
    "re.sub('[^A-Za-z가-힣]', '', string)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 5 biggest countries by population in 2017 are china, india, united states, indonesia, and brazil.\n"
     ]
    }
   ],
   "source": [
    "input_str = \"The 5 biggest countries by population in 2017 are China, India, United States, Indonesia, and Brazil.\"\n",
    "input_str = input_str.lower()\n",
    "print(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box A contains  red and  white balls, while Box B contains  red and  blue balls.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "input_str = \"Box A contains 3 red and 5 white balls, while Box B contains 4 red and 2 blue balls.\"\n",
    "result = re.sub(r'\\d+', '', input_str)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "The following code removes this set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of string with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "input_str = \"This &is [an] example? {of} string. with.? punctuation!!!!\" # Sample string\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "result = input_str.translate(translator)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a string example'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_str = \"\\t a string example\\t   \"\n",
    "input_str = input_str.strip()\n",
    "input_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(input_str)\n",
    "result = [i for i in tokens if not i in stop_words]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A scikit-learn tool  ,SpaCy also provides a stop words list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [i for i in tokens if not i in ENGLISH_STOP_WORDS]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [i for i in tokens if not i in STOP_WORDS]\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "* Stemming is a process of reducing words to their word stem, base or root form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before :  There\n",
      "after :  there\n",
      "====================\n",
      "before :  are\n",
      "after :  are\n",
      "====================\n",
      "before :  several\n",
      "after :  sever\n",
      "====================\n",
      "before :  types\n",
      "after :  type\n",
      "====================\n",
      "before :  of\n",
      "after :  of\n",
      "====================\n",
      "before :  stemming\n",
      "after :  stem\n",
      "====================\n",
      "before :  algorithms\n",
      "after :  algorithm\n",
      "====================\n",
      "before :  .\n",
      "after :  .\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "input_str= \"There are several types of stemming algorithms.\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(\"before : \" , word )\n",
    "    print(\"after : \" ,stemmer.stem(word))\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "* The aim of lemmatization, like stemming, is to reduce inflectional forms to a common base form.\n",
    "* As opposed to stemming, lemmatization does not simply chop off inflections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before :  been\n",
      "after :  been\n",
      "====================\n",
      "before :  had\n",
      "after :  had\n",
      "====================\n",
      "before :  done\n",
      "after :  done\n",
      "====================\n",
      "before :  languages\n",
      "after :  language\n",
      "====================\n",
      "before :  cities\n",
      "after :  city\n",
      "====================\n",
      "before :  mice\n",
      "after :  mouse\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "input_str= \"been had done languages cities mice\"\n",
    "input_str=word_tokenize(input_str)\n",
    "for word in input_str:\n",
    "    print(\"before : \", word)\n",
    "    print(\"after : \" , lemmatizer.lemmatize(word))\n",
    "    print(\"=\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of speech tagging (POS)\n",
    "* Part-of-speech tagging aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), ('an', 'DT'), ('article', 'NN'), ('to', 'TO'), ('write', 'VB'), ('interesting', 'VBG'), ('easily', 'RB'), ('and', 'CC'), ('of', 'IN')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "input_str = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking (shallow parsing)\n",
    "* Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'DT'), ('black', 'JJ'), ('television', 'NN'), ('and', 'CC'), ('a', 'DT'), ('white', 'JJ'), ('stove', 'NN'), ('were', 'VBD'), ('bought', 'VBN'), ('for', 'IN'), ('the', 'DT'), ('new', 'JJ'), ('apartment', 'NN'), ('of', 'IN'), ('John', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "input_str= \"A black television and a white stove were bought for the new apartment of John.\"\n",
    "from textblob import TextBlob\n",
    "result = TextBlob(input_str)\n",
    "print(result.tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The second step is chunking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT black/JJ television/NN)\n",
      "  and/CC\n",
      "  (NP a/DT white/JJ stove/NN)\n",
      "  were/VBD\n",
      "  bought/VBN\n",
      "  for/IN\n",
      "  (NP the/DT new/JJ apartment/NN)\n",
      "  of/IN\n",
      "  John/NNP)\n"
     ]
    }
   ],
   "source": [
    "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "rp = nltk.RegexpParser(reg_exp)\n",
    "result = rp.parse(result.tags)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition\n",
    "\n",
    "* Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Bill/NNP)\n",
      "  works/VBZ\n",
      "  for/IN\n",
      "  Apple/NNP\n",
      "  so/IN\n",
      "  he/PRP\n",
      "  went/VBD\n",
      "  to/TO\n",
      "  (GPE Boston/NNP)\n",
      "  for/IN\n",
      "  a/DT\n",
      "  conference/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
    "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
