{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Lec05 배울거 정말 많음","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"wosjVzxGutHI","colab_type":"text"},"cell_type":"markdown","source":["# https://docs.google.com/presentation/d/1SJMsa4BdOFVRCPD9uwaAqDBYYfgbQcbOm7HaxRVpaaY/edit#slide=id.g2f28335886_0_17"]},{"metadata":{"id":"8XSgOcWPPEHw","colab_type":"text"},"cell_type":"markdown","source":["# Name scope\n","\n","TensorFlow doesn’t know what nodes should be grouped together, unless you tell it to\n","\n","Group nodes together with tf.name_scope(name)\n","\n","\n","```\n","with tf.name_scope(name_of_that_scope):\n","\t# declare op_1\n","\t# declare op_2\n","```\n","\n","\n","\n"]},{"metadata":{"id":"v8ubcLWDPNza","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","```\n","with tf.name_scope('data'):\n","    iterator = dataset.make_initializable_iterator()\n","    center_words, target_words = iterator.get_next()\n","with tf.name_scope('embed'):\n","    embed_matrix = tf.get_variable('embed_matrix', \n","                                    shape=[VOCAB_SIZE, EMBED_SIZE], ...)\n","    embed = tf.nn.embedding_lookup(embed_matrix, center_words)\n","with tf.name_scope('loss'):\n","    nce_weight = tf.get_variable('nce_weight', shape=[VOCAB_SIZE, EMBED_SIZE], ...)\n","    nce_bias = tf.get_variable('nce_bias', initializer=tf.zeros([VOCAB_SIZE]))\n","    loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weight, biases=nce_bias, …)\n","with tf.name_scope('optimizer'):\n","    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE).minimize(loss)\n","\n","```\n","\n"]},{"metadata":{"id":"ddjJZbn1PaWd","colab_type":"text"},"cell_type":"markdown","source":["## Name scope vs variable scope\n","\n","tf.name_scope() vs tf.variable_scope()\n","\n","Variable scope facilitates variable sharing\n","\n","\n","\n","```\n","def two_hidden_layers(x):\n","    w1 = tf.Variable(tf.random_normal([100, 50]), name='h1_weights')\n","    b1 = tf.Variable(tf.zeros([50]), name='h1_biases')\n","    h1 = tf.matmul(x, w1) + b1\n","\n","    w2 = tf.Variable(tf.random_normal([50, 10]), name='h2_weights')\n","    b2 = tf.Variable(tf.zeros([10]), name='2_biases')\n","    logits = tf.matmul(h1, w2) + b2\n","    return logits\n","\n","logits1 = two_hidden_layers(x1)\n","logits2 = two_hidden_layers(x2)\n","\n","```\n","# # What will happen if we make these two calls?\n","\n","Sharing Variable: The problem\n","\n","Two sets of variables are created.\n","\n","You want all your inputs to use the same weights and biases!\n","\n","\n","### solution\n","```\n"," tf.get_variable( <name>, <shape>, <initializer>)\n","```\n","\n","If a variable with <name> already exists, reuse it\n","If not, initialize it with <shape> using <initializer>\n","  \n","```\n","def two_hidden_layers(x):\n","    assert x.shape.as_list() == [200, 100]\n","    w1 = tf.get_variable(\"h1_weights\", [100, 50], initializer=tf.random_normal_initializer())\n","    b1 = tf.get_variable(\"h1_biases\", [50], initializer=tf.constant_initializer(0.0))\n","    h1 = tf.matmul(x, w1) + b1\n","    assert h1.shape.as_list() == [200, 50]  \n","    w2 = tf.get_variable(\"h2_weights\", [50, 10], initializer=tf.random_normal_initializer())\n","    b2 = tf.get_variable(\"h2_biases\", [10], initializer=tf.constant_initializer(0.0))\n","    logits = tf.matmul(h1, w2) + b2\n","    return logits\n","logits1 = two_hidden_layers(x1)\n","logits2 = two_hidden_layers(x2)\n","\n","```\n","\n","## problem\n","ValueError: Variable h1_weights already exists, disallowed. Did you mean to set reuse=True in VarScope?\n","\n","```\n","with tf.variable_scope('two_layers') as scope:\n","    logits1 = two_hidden_layers(x1)\n","    scope.reuse_variables()\n","    logits2 = two_hidden_layers(x2)\n","\n","```\n","Put your variables within a scope and reuse all variables within that scope\n","\n","Only one set of variables, all within the variable scope “two_layers”\n","\n","They take in two different inputs\n","\n","tf.variable_scope implicitly creates a name scope\n","\n","## Reusable code?\n","\n","```\n","def two_hidden_layers(x):\n","    assert x.shape.as_list() == [200, 100]\n","    w1 = tf.get_variable(\"h1_weights\", [100, 50], initializer=tf.random_normal_initializer())\n","    b1 = tf.get_variable(\"h1_biases\", [50], initializer=tf.constant_initializer(0.0))\n","    h1 = tf.matmul(x, w1) + b1\n","    assert h1.shape.as_list() == [200, 50]  \n","    w2 = tf.get_variable(\"h2_weights\", [50, 10], initializer=tf.random_normal_initializer())\n","    b2 = tf.get_variable(\"h2_biases\", [10], initializer=tf.constant_initializer(0.0))\n","    logits = tf.matmul(h1, w2) + b2\n","    return logits\n","with tf.variable_scope('two_layers') as scope:\n","    logits1 = two_hidden_layers(x1)\n","    scope.reuse_variables()\n","    logits2 = two_hidden_layers(x2)\n","\n","```\n","## layer'em up\n","Fetch variables if they already exist \n","\n","Else, create them\n","\n","```\n","def fully_connected(x, output_dim, scope):\n","    with tf.variable_scope(scope, reuse=tf.AUTO_REUSE) as scope:  # reuse=tf.AUTO_REUSE\n","        w = tf.get_variable(\"weights\", [x.shape[1], output_dim], initializer=tf.random_normal_initializer())\n","        b = tf.get_variable(\"biases\", [output_dim], initializer=tf.constant_initializer(0.0))\n","        return tf.matmul(x, w) + b\n","\n","def two_hidden_layers(x):\n","    h1 = fully_connected(x, 50, 'h1')\n","    h2 = fully_connected(h1, 10, 'h2')\n","\n","with tf.variable_scope('two_layers') as scope:\n","    logits1 = two_hidden_layers(x1)\n","    logits2 = two_hidden_layers(x2)\n","```\n"]},{"metadata":{"id":"mlUZ8tf2_FtE","colab_type":"text"},"cell_type":"markdown","source":["# Graph collections\n"," As you create a model, you might put your variables to different parts of the graph\n","you’d want an easy way to access them\n","\n","```\n","\n","tf.get_collection(\n","    key,\n","    scope=None\n",")\n","\n","```\n","## tf.get_collection lets you access a certain collection of variables, with key being the name of the collection, scope is the scope of the variables.\n","\n","By default, all variables are placed in tf.GraphKeys.GLOBAL_VARIABLES. \n","\n","To get all variables in scope “my_scope”, simply call. This turns a list of variables in “my_scope”.\n","\n","```\n","tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='my_scope')\n","```\n","\n","If you set trainable=True (which is always set by default) when you create your variable, \n","\n","that variable will be in the collection tf.GraphKeys.TRAINABLE_VARIABLES. \n","\n"]},{"metadata":{"id":"1w4usV9Vruh3","colab_type":"text"},"cell_type":"markdown","source":["# Manage Experiments\n","\n","tf.train.Saver\n","saves graph’s variables in binary files\n","\n","Saves sessions, not graphs!\n","\n","```\n","tf.train.Saver.save(sess, save_path, global_step=None...)\n","tf.train.Saver.restore(sess, save_path)\n","\n","```\n","## Save parameters after 1000 steps\n","```\n","# define model\n","model = SkipGramModel(params)\n","\n","# create a saver object\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","\tfor step in range(training_steps): \n","\t\tsess.run([optimizer])\n","\t\t\n","\t\t# save model every 1000 steps\n","\t\tif (step + 1) % 1000 == 0:\n","\t\t\tsaver.save(sess,  'checkpoint_directory/model_name', global_step=step)\n","\n","```\n","## Specify the step at which the model is saved\n","```\n","# define model\n","model = SkipGramModel(params)\n","\n","# create a saver object\n","saver = tf.train.Saver()\n","\n","with tf.Session() as sess:\n","\tfor step in range(training_steps): \n","\t\tsess.run([optimizer])\n","\t\t\n","\t\t# save model every 1000 steps\n","\t\tif (step + 1) % 1000 == 0:\n","\t\t\tsaver.save(sess,  'checkpoint_directory/model_name', global_step=step)\n","\n","```\n","## Global step\n","Very common in TensorFlow program\n","\n","```\n","global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n","\n","global_step = tf.Variable(0,  dtype=tf.int32,  trainable=False,   name='global_step')\n","\n","optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n","\n","```\n","Need to tell optimizer to increment global step\n","\n","This can also help your optimizer know when to decay learning rate\n","\n","### tf.train.Saver\n","Only save variables, not graph\n","Checkpoints map variable names to tensors\n","\n","```\n","# Can also choose to save certain variables\n","v1 = tf.Variable(..., name='v1') \n","v2 = tf.Variable(..., name='v2') \n","\n","# You can save your variables in one of three ways:\n","saver = tf.train.Saver({'v1': v1, 'v2': v2})\n","saver = tf.train.Saver([v1, v2])\n","saver = tf.train.Saver({v.op.name: v for v in [v1, v2]}) # similar to a dict\n","\n","```\n","## Restore variables\n","\n","Still need to first build graph\n","\n","```\n","aver.restore(sess, 'checkpoints/name_of_the_checkpoint')\n","\n","e.g. saver.restore(sess, 'checkpoints/skip-gram-99999')\n","\n","```\n","### Restore the latest checkpoint\n","\n","\n","```\n","# check if there is checkpoint\n","\n","ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","\n","# check if there is a valid checkpoint path\n","\n","if ckpt and ckpt.model_checkpoint_path:\n","     saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","\n","```\n","1.checkpoint file keeps track of the latest checkpoint\n","\n","2.restore checkpoints only when there is a valid checkpoint path\n","\n","\n","\n","\n"]},{"metadata":{"id":"MeEhJhdls0SU","colab_type":"text"},"cell_type":"markdown","source":["# tf.summary\n","\n","Why matplotlib when you can summarize?\n","\n","Visualize our summary statistics during our training\n","\n","tf.summary.scalar\n","\n","tf.summary.histogram\n","\n","tf.summary.image\n","\n","\n","---\n","\n","## Step 1: create summaries\n","\n","\n","```\n","with tf.name_scope(\"summaries\"):\n","    tf.summary.scalar(\"loss\", self.loss)\n","    tf.summary.scalar(\"accuracy\", self.accuracy)            \n","    tf.summary.histogram(\"histogram loss\", self.loss)\n","    summary_op = tf.summary.merge_all()\n","\n","```\n","merge them all into one summary op to make managing them easier\n","\n","\n","---\n","## Step 2: run them\n","Like everything else in TF, summaries are ops. \n","For the summaries to be built, you have to run it in a session\n","\n","```\n","loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n","\n","```\n","\n","\n","---\n","\n","## Step 3: write summaries to file\n","Need global step here so the model knows what summary corresponds to what step\n","\n","```\n","writer.add_summary(summary, global_step=step)\n","```\n","---\n","\n","## Putting it together\n","```\n","tf.summary.scalar(\"loss\", self.loss)\n","tf.summary.histogram(\"histogram loss\", self.loss)\n","summary_op = tf.summary.merge_all()\n","\n","saver = tf.train.Saver() # defaults to saving all variables\n","\n","with tf.Session() as sess:\n","    sess.run(tf.global_variables_initializer())\n","    ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/checkpoint'))\n","    if ckpt and ckpt.model_checkpoint_path:\n","        saver.restore(sess, ckpt.model_checkpoint_path)\n","\n","    writer = tf.summary.FileWriter('./graphs', sess.graph)\n","    for index in range(10000):\n","        ...\n","        loss_batch, _, summary = sess.run([loss, optimizer, summary_op])\n","        writer.add_summary(summary, global_step=index)\n","\n","        if (index + 1) % 1000 == 0:\n","            saver.save(sess, 'checkpoints/skip-gram', index)\n","\n","```\n","\n","\n","\n","\n","\n"]},{"metadata":{"id":"tyN_rerQuF6U","colab_type":"text"},"cell_type":"markdown","source":["# Control Randomization\n","\n","## Op level random seed\n","\n","```\n","my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0)) # seed\n","```\n","## Sessions keep track of random state\n","Each new session restarts the random state\n","\n","```\n","c = tf.random_uniform([], -10, 10, seed=2)\n","\n","with tf.Session() as sess:\n","\tprint(sess.run(c)) # >> 3.57493\n","\tprint(sess.run(c)) # >> -5.97319\n","\n","--------------------------------------------\n","c = tf.random_uniform([], -10, 10, seed=2)\n","\n","with tf.Session() as sess:\n","\tprint(sess.run(c)) # >> 3.57493\n","\n","with tf.Session() as sess:\n","\tprint(sess.run(c)) # >> 3.57493\n","\n","```\n","## Op level seed: each op keeps its own seed\n","\n","```\n","c = tf.random_uniform([], -10, 10, seed=2)\n","d = tf.random_uniform([], -10, 10, seed=2)\n","\n","with tf.Session() as sess:\n","\tprint(sess.run(c)) # >> 3.57493\n","\tprint(sess.run(d)) # >> 3.5749\n","\n","```\n","## Graph level seed\n","Note that the result is different from op-level seed\n","\n","```\n","tf.set_random_seed(2) # tf.set_random_seed(2)\n","c = tf.random_uniform([], -10, 10)\n","d = tf.random_uniform([], -10, 10)\n","\n","with tf.Session() as sess:\n","    print(sess.run(c)) # >> -4.00752\n","    print(sess.run(d)) # >> -2.98339\n","\n","```\n","\n","\n","\n"]},{"metadata":{"id":"KZPX-rwyupAv","colab_type":"text"},"cell_type":"markdown","source":["# Autodiff\n","\n","## Where are the gradients?\n","## TensorFlow builds the backward path for you!\n","## Reverse mode automatic differentiation\n","The computation graph makes computing symbolic gradients straightforward\n","\n","Chain rule\n","\n","# tf.gradients(y, [xs])\n","Take derivative of y with respect to each tensor in the list [xs]\n","\n","```\n","x = tf.Variable(2.0)\n","y = 2.0 * (x ** 3)\n","z = 3.0 + y ** 2\n","grad_z = tf.gradients(z, [x, y])\n","with tf.Session() as sess:\n","\tsess.run(x.initializer)\n","\tprint(sess.run(grad_z)) # >> [768.0, 32.0]\n","# 768 is the gradient of z with respect to x, 32 with respect to y\n","\n","```\n","## Gradient Computation\n","\n","```\n","tf.gradients(ys, xs, grad_ys=None, ...)\n","tf.stop_gradient(input, name=None)\n","\n","# prevents the contribution of its inputs to be taken into account\n","tf.clip_by_value(t, clip_value_min, clip_value_max, name=None)\n","tf.clip_by_norm(t, clip_norm, axes=None, name=None)\n","\n","```\n","## Should I still learn to take gradients?\n"]},{"metadata":{"id":"hCoaOKAZQXCI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"Oyrvbt75QMA0","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"D8EgLCyeSWnP","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"e9PuwwLMSXEP","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8nGtS7P7PiRx","colab_type":"text"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"Z58UyxjJPDGm","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"JQ_n3bKyPQOo","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}