{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LEC06 CONV.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]}},"cells":[{"metadata":{"id":"ovK3aOMS7bBO","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8DOqhY437dld","colab_type":"text"},"cell_type":"markdown","source":["# Convolutional Neural Networks:Deep Learning with Images\n","\n","Object Segmentation\n","\n","Pose Estimation \n","\n","Image Captioning\n","\n","Dense Image Captioning\n","\n","Visual Question Answering\n","\n","Image Super-resolution\n","\n","Art generation\n","\n"]},{"metadata":{"id":"s7Bq4r_N8Fw3","colab_type":"text"},"cell_type":"markdown","source":["# Convolutional Neural Networks\n","\n","Recall: fully connected neural network\n","\n","Convolving “filters” is not a new idea\n","\n","## Two key insights\n","1. Features are hierarchical\n","   Composing high-complexity features out of low-complexity features is more efficient than learning high-complexity features directly.\n","\te.g.: having an “circle” detector is useful for detecting faces… and basketballs\n","2. Features are translationally invariant(불변)\n","    If a feature is useful to compute at (x, y) it is useful to compute that feature at (x’, y’) as well\n"]},{"metadata":{"id":"yXEDZJRuaFZG","colab_type":"text"},"cell_type":"markdown","source":["# Convolutional Layer\n","\n","## Overview and intuition without brain stuff\n","\n","## Local Connectivity\n"," Instead, we will connect each neuron to only a local region of the input volume\n"," \n","  The spatial extent of this connectivity is a hyperparameter called the receptive field of the neuron (equivalently this is the filter size).\n","  \n"," ## Spatial arrangement\n"," \n"," Three hyperparameters control the size of the output volume: the depth, stride and zero-padding\n","  depth \n","  S : STRIDE  W :  the receptive field size of the Conv Layer neurons \n","  \n","  P : the amount of zero padding \n","  \n","  ## (W - F + 2P)/S + 1   P = (F - 1)/2\n","  \n","  ## Parameter Sharing\n","  e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55]), we are going to constrain the neurons in each depth slice to use the same weights and bias\n"," \n"," ## Backpropagation.\n"," \n"," ## 1x1 convolution\n"," \n"," 1x1 convolutions would effectively be doing 3-dimensional dot products (since the input depth is 3 channels)\n"," \n"," ## Pooling Layer\n"," Its function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence to also control overfitting\n"," \n"," ## Fully-connected layer\n"," \n"," "]},{"metadata":{"id":"jbhqw8oBG4pa","colab_type":"text"},"cell_type":"markdown","source":["# parameter sharing 부터  Lecture note"]}]}