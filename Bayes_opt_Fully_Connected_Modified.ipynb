{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BayesianOptimization in Tensorflow\n",
    "\n",
    "###  focal loss 적용 [Url](https://github.com/fudannlp16/focal-loss/blob/master/focal_loss.py)\n",
    "    * 추가적으로 alpha weight를 Parameter로 넣었음\n",
    "    \n",
    "### 다양한 Tensorflow에서 제공하는 Loss [Url](https://www.tensorflow.org/api_docs/python/tf/losses/sparse_softmax_cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bayesian-optimization --user\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection  import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 3. 4.]\n",
      "9.0\n",
      "1.45\n",
      "14.5\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([2.0,3.0,4.0])\n",
    "b = tf.contrib.layers.l1_regularizer(scale= 1.0)(a)\n",
    "c = tf.contrib.layers.l2_regularizer(scale= 0.1)(a)\n",
    "d = tf.nn.l2_loss(a)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    #输出L1 正则化，计算方法（|2.0|+|3.0|+|4.0|）*1 = 9\n",
    "    print(sess.run(b))\n",
    "    #输出L2 正则化(2^2+3^2+4^2)*1/2 = 14.5\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./../credit44_sc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT_CONTACT_POS\n",
      "8\n",
      "[4 3 2 1 0 7 5 6]\n",
      "CNT_ENG\n",
      "7\n",
      "[4 3 1 0 2 6 5]\n",
      "DAYS_CONTACT_POS\n",
      "11\n",
      "[   0    3    1 7777    4    5    2    6    8    7    9]\n",
      "DAYS_CALL_PAYMENT\n",
      "11\n",
      "[   0    3    1 7777    4    5    2    6    8    7    9]\n",
      "EWS_C_N_P27000100\n",
      "14\n",
      "[ 3  4  1  0  2  5 10  8  7  6  9 12 13 11]\n",
      "EWS_C_N_P42000200\n",
      "14\n",
      "[ 3  2  0  1  5 10  6  4  7  8  9 11 13 12]\n",
      "EWS_C_N_P32002600\n",
      "22\n",
      "[ 6  7  0  1  5  3  2 11 14  8  4  9 12 10 17 16 18 15 13 21 20 19]\n",
      "C_N_PS0001777\n",
      "11\n",
      "[ 3  4  1  0  5 10  2  6  7  8  9]\n",
      "A_K_D10220000_OPR\n",
      "9\n",
      "[2 3 0 5 1 4 8 6 7]\n",
      "DAYS_CONTACT\n",
      "7\n",
      "[0 2 1 5 3 6 4]\n",
      "MOB\n",
      "14\n",
      "[12 13 11 14 10  9  8  7  6  5  4  3  2  1]\n",
      "EWS_C_K_D10220000_OPR\n",
      "9\n",
      "[2 3 0 5 1 4 8 6 7]\n",
      "EWS_C_K_D10210D00_OPR\n",
      "11\n",
      "[ 2  3  0  5  1  4  8  6  7  9 10]\n"
     ]
    }
   ],
   "source": [
    "categorical = []\n",
    "for i in list(df)[:-1] :\n",
    "    count = df[i].nunique()\n",
    "    if count < 25 :\n",
    "        print(i)\n",
    "        print(df[i].nunique())\n",
    "        print(df[i].unique())\n",
    "        categorical.append(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9818, 45)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = list(set(list(df)) - set(categorical) - {\"target\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gpuadmin/.local/lib/python3.5/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df[numerical])\n",
    "df[numerical] = scaler.transform(df[numerical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CNT_CONTACT_POS',\n",
       " 'CNT_ENG',\n",
       " 'DAYS_CONTACT_POS',\n",
       " 'DAYS_CALL_PAYMENT',\n",
       " 'EWS_C_N_P27000100',\n",
       " 'EWS_C_N_P42000200',\n",
       " 'EWS_C_N_P32002600',\n",
       " 'C_N_PS0001777',\n",
       " 'A_K_D10220000_OPR',\n",
       " 'DAYS_CONTACT',\n",
       " 'MOB',\n",
       " 'EWS_C_K_D10220000_OPR',\n",
       " 'EWS_C_K_D10210D00_OPR']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT_CONTACT_POS\n",
      "8\n",
      "[4 3 2 1 0 7 5 6]\n",
      "CNT_ENG\n",
      "7\n",
      "[4 3 1 0 2 6 5]\n",
      "DAYS_CONTACT_POS\n",
      "11\n",
      "[   0    3    1 7777    4    5    2    6    8    7    9]\n",
      "DAYS_CALL_PAYMENT\n",
      "11\n",
      "[   0    3    1 7777    4    5    2    6    8    7    9]\n",
      "EWS_C_N_P27000100\n",
      "14\n",
      "[ 3  4  1  0  2  5 10  8  7  6  9 12 13 11]\n",
      "EWS_C_N_P42000200\n",
      "14\n",
      "[ 3  2  0  1  5 10  6  4  7  8  9 11 13 12]\n",
      "EWS_C_N_P32002600\n",
      "22\n",
      "[ 6  7  0  1  5  3  2 11 14  8  4  9 12 10 17 16 18 15 13 21 20 19]\n",
      "C_N_PS0001777\n",
      "11\n",
      "[ 3  4  1  0  5 10  2  6  7  8  9]\n",
      "A_K_D10220000_OPR\n",
      "9\n",
      "[2 3 0 5 1 4 8 6 7]\n",
      "DAYS_CONTACT\n",
      "7\n",
      "[0 2 1 5 3 6 4]\n",
      "MOB\n",
      "14\n",
      "[12 13 11 14 10  9  8  7  6  5  4  3  2  1]\n",
      "EWS_C_K_D10220000_OPR\n",
      "9\n",
      "[2 3 0 5 1 4 8 6 7]\n",
      "EWS_C_K_D10210D00_OPR\n",
      "11\n",
      "[ 2  3  0  5  1  4  8  6  7  9 10]\n"
     ]
    }
   ],
   "source": [
    "for i in categorical :\n",
    "    print(i)\n",
    "    print(df[i].nunique())\n",
    "    print(df[i].unique())\n",
    "    one_hot = pd.get_dummies(df[i] , drop_first  = False , prefix = str(i)+\"_\")\n",
    "    # Drop column B as it is now encoded\n",
    "    df = df.drop(i,axis = 1)\n",
    "    # Join the encoded df\n",
    "    df = df.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9818, 180)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = df.drop(\"target\", axis = 1 ).values.astype(np.float32)\n",
    "y_datalabel = df.loc[:, \"target\"]\n",
    "y_data = LabelEncoder().fit_transform(df.loc[:, \"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = np.zeros((y_data.shape[0], np.unique(y_data).shape[0]))\n",
    "for i in range(y_data.shape[0]):\n",
    "    onehot[i, y_data[i]] = 1.0\n",
    "x_train, x_test, y_train, y_test, y_train_label, y_test_label = train_test_split(x_data, onehot, y_data, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6872, 179)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss_sigmoid(labels,logits,alpha=0.25 , gamma=2):\n",
    "    \"\"\"\n",
    "    Computer focal loss for binary classification\n",
    "    Args:\n",
    "      labels: A int32 tensor of shape [batch_size].\n",
    "      logits: A float32 tensor of shape [batch_size].\n",
    "      alpha: A scalar for focal loss alpha hyper-parameter. If positive samples number\n",
    "      > negtive samples number, alpha < 0.5 and vice versa.\n",
    "      gamma: A scalar for focal loss gamma hyper-parameter.\n",
    "    Returns:\n",
    "      A tensor of the same shape as `lables`\n",
    "    \"\"\"\n",
    "    y_pred=tf.nn.sigmoid(logits)\n",
    "    labels=tf.to_float(labels)\n",
    "    L=-labels*(1-alpha)*((1-y_pred)*gamma)*tf.log(  tf.maximum(y_pred , 1e-8)   )-\\\n",
    "      (1-labels)*alpha*(y_pred**gamma)*tf.log( tf.maximum( 1-y_pred ,  1e-8 ) ) \n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_regularizer = tf.contrib.layers.l1_regularizer(scale=0.005, scope=None)\n",
    "\n",
    "def neural_network(num_hidden, size_layer, learning_rate , dropout_rate , beta , activation , focal_weight , reduction_node , batch_size = 200  ):\n",
    "    \n",
    "    def activate(activation, first_layer, second_layer, bias):\n",
    "        if activation == 0:\n",
    "            activation = tf.nn.leaky_relu\n",
    "        elif activation == 1:\n",
    "            activation = tf.nn.tanh\n",
    "        else:\n",
    "            activation = tf.nn.relu\n",
    "        layer = activation(tf.matmul(first_layer, second_layer) + bias)\n",
    "        return tf.contrib.nn.alpha_dropout(layer, dropout_rate)\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    X = tf.placeholder(tf.float32, (None, x_data.shape[1]))\n",
    "    Y = tf.placeholder(tf.float32, (None, onehot.shape[1]))\n",
    "    ## W = tf.Variable(tf.contrib.layers.xavier_initializer()((n_prev, n)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    input_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((x_data.shape[1], size_layer)))\n",
    "    biased_layer = tf.Variable(tf.random_normal([size_layer], stddev = 0.1))\n",
    "    output_layer = tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer - reduction_node * (num_hidden - 1), onehot.shape[1])))\n",
    "    biased_output = tf.Variable(tf.random_normal([onehot.shape[1]], stddev = 0.1))\n",
    "    \n",
    "    \n",
    "    layers, biased = [], []\n",
    "    \n",
    "#     for i in range(num_hidden - 1):\n",
    "#         layers.append(tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer, size_layer))))\n",
    "#         biased.append(tf.Variable(tf.random_normal([size_layer])))\n",
    "     \n",
    "    for i in range(num_hidden - 1):\n",
    "        size_layer2 = size_layer - reduction_node \n",
    "        layers.append(tf.Variable(tf.contrib.layers.xavier_initializer()((size_layer, size_layer2))))        \n",
    "        biased.append(tf.Variable(tf.random_normal([size_layer2])))\n",
    "        size_layer = size_layer2\n",
    "    \n",
    "    \n",
    "    \n",
    "    first_l = activate(activation, X, input_layer, biased_layer)\n",
    "    next_l = activate(activation, first_l, layers[0], biased[0])\n",
    "    \n",
    "    for i in range(1, num_hidden - 1):\n",
    "        next_l = activate(activation, next_l, layers[i], biased[i])\n",
    "    \n",
    "    last_l = tf.matmul(next_l, output_layer) + biased_output\n",
    "    cost2 = tf.reduce_mean( focal_loss_sigmoid(logits = last_l, labels = Y , alpha = focal_weight ))\n",
    "    # tf.nn.softmax_cross_entropy_with_logits_v2(logits = last_l, labels = Y)\n",
    "    \n",
    "    #regularizers = tf.nn.l2_loss(input_layer) + sum(map(lambda x: tf.nn.l2_loss(x), layers)) + tf.nn.l2_loss(output_layer)\n",
    "    regularizers = tf.contrib.layers.l1_l2_regularizer(0.5, 0.5)(input_layer) + \\\n",
    "    sum(map(lambda x: tf.contrib.layers.l1_l2_regularizer(0.5, 0.5)(x), layers)) \n",
    "    #tf.contrib.layers.l1_l2_regularizer(1.0, 1.0)(input_layer) + \n",
    "    \n",
    "    \n",
    "    cost = cost2 + beta * regularizers\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "    correct_prediction = tf.equal(tf.argmax(last_l, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    \n",
    "    sess = tf.InteractiveSession()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    COST, TEST_COST, ACC, TEST_ACC = [], [], [], []\n",
    "    \n",
    "    for i in range(1000):\n",
    "        train_acc, train_loss = 0, 0\n",
    "        for n in range(0, (x_train.shape[0] // batch_size) * batch_size, batch_size):\n",
    "            _, loss , loss2 , regu2 = sess.run([optimizer, cost , cost2 , regularizers ], \n",
    "                                                 feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n",
    "            train_acc += sess.run(accuracy, feed_dict = {X: x_train[n: n + batch_size, :], Y: y_train[n: n + batch_size, :]})\n",
    "            train_loss += loss\n",
    "        \n",
    "        if i % 50 == 0 :\n",
    "            print(\"Epoch : {} , softmax loss : {} , Regularizer : {} , Total Loss : {}\".format(i ,loss2 , regu2 , loss))\n",
    "\n",
    "        TEST_COST.append(sess.run(cost, feed_dict = {X: x_test, Y: y_test}))\n",
    "        TEST_ACC.append(sess.run(accuracy, feed_dict = {X: x_test, Y: y_test}))\n",
    "        train_loss /= (x_train.shape[0] // batch_size)\n",
    "        train_acc /= (x_train.shape[0] // batch_size)\n",
    "        ACC.append(train_acc)\n",
    "        COST.append(train_loss)\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    COST = np.array(COST).mean()\n",
    "    TEST_COST = np.array(TEST_COST).mean()\n",
    "    ACC = np.array(ACC).mean()\n",
    "    TEST_ACC = np.array(TEST_ACC).mean()\n",
    "    return COST, TEST_COST, ACC, TEST_ACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def generate_nn(num_hidden, size_layer, learning_rate, dropout_rate, beta, activation , focal_weight , reduction_node ):\n",
    "    global accbest\n",
    "    param = {\n",
    "        'num_hidden' : int(np.around(num_hidden)),\n",
    "        'size_layer' : int(np.around(size_layer)),\n",
    "        'learning_rate' : max(min(learning_rate, 1), 0.0001),\n",
    "        'dropout_rate' : max(min(dropout_rate, 0.3), 0),\n",
    "        'beta' : max(min(beta, 0.5), 0.000001),\n",
    "        'activation': int(np.around(activation)) , \n",
    "        \"focal_weight\" : max( min(focal_weight , 0.5) , 0.01) ,\n",
    "        \"reduction_node\" : min( int(reduction_node) , int(np.around(size_layer) / np.around(num_hidden) )-3  ) ,\n",
    "    }\n",
    "    print(\"\\n Search parameters \\n %s\" % (param), file = log_file)\n",
    "    \n",
    "    log_file.flush()\n",
    "    learning_cost, valid_cost, learning_acc, valid_acc = neural_network(**param)\n",
    "    print(\"stop after 500 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f\" % (learning_cost, valid_cost, learning_acc, valid_acc))\n",
    "    \n",
    "    f = open(\"nn-bayesian_acc.txt\",'a')\n",
    "    result_ = \"stop after 500 iteration with train cost %f, valid cost %f, train acc %f, valid acc %f \\n\" % (learning_cost, valid_cost, learning_acc, valid_acc)\n",
    "    f.write(result_)\n",
    "    if (valid_acc > accbest):\n",
    "        costbest = valid_acc\n",
    "    return valid_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acquistion Function [EXAMPLE](https://github.com/fmfn/BayesianOptimization/blob/master/examples/exploitation_vs_exploration.ipynb)\n",
    "\n",
    "* Expected Improvement[ei] Prefer exploitation (xi=0.0)\n",
    "* Probability of Improvement[poi] Prefer exploitation (xi=0.0)\n",
    "* \"Upper Confidence Bound\" [ucb] Prefer exploitation (kappa=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop after 500 iteration with train cost 0.168402, valid cost 0.163471, train acc 0.621907, valid acc 0.628444\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.6284  \u001b[0m | \u001b[0m 0.3461  \u001b[0m | \u001b[0m 0.002901\u001b[0m | \u001b[0m 0.2516  \u001b[0m | \u001b[0m 0.2135  \u001b[0m | \u001b[0m 0.001553\u001b[0m | \u001b[0m 5.408   \u001b[0m | \u001b[0m 33.93   \u001b[0m | \u001b[0m 167.7   \u001b[0m |\n",
      "Epoch : 0 , softmax loss : 0.25727108120918274 , Regularizer : 467.522705078125 , Total Loss : 0.6403969526290894\n",
      "Epoch : 50 , softmax loss : 0.11038479208946228 , Regularizer : 9.870382308959961 , Total Loss : 0.11847338080406189\n",
      "Epoch : 100 , softmax loss : 0.10886649787425995 , Regularizer : 9.92916488647461 , Total Loss : 0.11700325459241867\n",
      "Epoch : 150 , softmax loss : 0.10877034813165665 , Regularizer : 9.830238342285156 , Total Loss : 0.11682604253292084\n",
      "Epoch : 200 , softmax loss : 0.1089518666267395 , Regularizer : 9.9520902633667 , Total Loss : 0.11710741370916367\n",
      "Epoch : 250 , softmax loss : 0.10940582305192947 , Regularizer : 9.98249626159668 , Total Loss : 0.11758628487586975\n",
      "Epoch : 300 , softmax loss : 0.10951535403728485 , Regularizer : 9.800275802612305 , Total Loss : 0.11754649132490158\n",
      "Epoch : 350 , softmax loss : 0.10836109519004822 , Regularizer : 9.87806224822998 , Total Loss : 0.11645597964525223\n",
      "Epoch : 400 , softmax loss : 0.10809867829084396 , Regularizer : 9.956266403198242 , Total Loss : 0.11625764518976212\n",
      "Epoch : 450 , softmax loss : 0.10957865417003632 , Regularizer : 9.857362747192383 , Total Loss : 0.11765657365322113\n",
      "Epoch : 500 , softmax loss : 0.10923559963703156 , Regularizer : 9.863288879394531 , Total Loss : 0.11731837689876556\n",
      "Epoch : 550 , softmax loss : 0.10858634114265442 , Regularizer : 9.944601058959961 , Total Loss : 0.11673574894666672\n",
      "Epoch : 600 , softmax loss : 0.10835225880146027 , Regularizer : 9.808815002441406 , Total Loss : 0.11639039218425751\n",
      "Epoch : 650 , softmax loss : 0.1085270419716835 , Regularizer : 9.825315475463867 , Total Loss : 0.11657869815826416\n"
     ]
    }
   ],
   "source": [
    "log_file = open('nn-bayesian.log', 'a')\n",
    "accbest = 0.8\n",
    "NN_BAYESIAN = BayesianOptimization(generate_nn, \n",
    "                              {'num_hidden': (3, 6),\n",
    "                               'size_layer': (120 , 170),\n",
    "                               'learning_rate': (0.001, 0.005),\n",
    "                               'dropout_rate': (0.1, 0.3),\n",
    "                               'beta': (0.0001, 0.01),\n",
    "                               'activation': (0, 2),\n",
    "                               \"focal_weight\" : (0.1 , 0.4),\n",
    "                               \"reduction_node\" : (25 , 45)\n",
    "                              })\n",
    "NN_BAYESIAN.maximize(init_points = 100 , n_iter = 1000 , acq=\"ucb\", kappa= 10.0)\n",
    "\n",
    "## ucb xi = 0.1\n",
    "# acq=\"poi\", xi=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Maximum NN accuracy value: %f' % NN_BAYESIAN.res['max']['max_val'])\n",
    "print('Best NN parameters: ', NN_BAYESIAN.res['max']['max_params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [시각화 예제](https://github.com/fmfn/BayesianOptimization/blob/master/examples/visualization.ipynb)\n",
    "## [sklearn 적용 예시](https://github.com/fmfn/BayesianOptimization/blob/master/examples/sklearn_example.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
