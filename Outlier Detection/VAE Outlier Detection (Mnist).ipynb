{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn as sk\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "import warnings , os\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## https://github.com/nakumgaurav/Anomaly-Detection_Varitaional-Autoencoders\n",
    "\n",
    "### [tf.flags](https://daeson.tistory.com/256)\n",
    "* int ,float , boolean , string 값 저장해서 쉽게 사용 할 수 있게 해주는 것!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "distributions = tf.distributions\n",
    "\n",
    "####Delete all flags before declare#####\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:    \n",
    "        FLAGS.__delattr__(keys)\n",
    "        \n",
    "del_all_flags(tf.flags.FLAGS)\n",
    "tf.reset_default_graph()\n",
    "\n",
    "flags = tf.app.flags\n",
    "## 현재 디렉토리를 만드는 듯?\n",
    "flags.DEFINE_string('data_dir', 'data/', 'Directory for data')\n",
    "flags.DEFINE_string('logdir', '/home/advice/Python/SR/board/VAE_OD', 'Directory for logs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags.DEFINE_integer('latent_dim', 2, 'Latent dimensionality of model')\n",
    "flags.DEFINE_integer('batch_size', 256 , 'Minibatch size')\n",
    "flags.DEFINE_integer('visual_size', 1000 , 'visualization size')\n",
    "flags.DEFINE_integer('n_samples', 1, 'Number of samples to save')\n",
    "flags.DEFINE_integer('print_every', 100, 'Print every n iterations')\n",
    "flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "flags.DEFINE_integer('n_iterations', 1000, 'number of iterations')\n",
    "flags.DEFINE_float('threshold', 7.0, 'Threshold for recon probab')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_dir': <absl.flags._flag.Flag at 0x7f8363daada0>,\n",
       " 'logdir': <absl.flags._flag.Flag at 0x7f8363daa940>,\n",
       " 'latent_dim': <absl.flags._flag.Flag at 0x7f8363dc8240>,\n",
       " 'batch_size': <absl.flags._flag.Flag at 0x7f8363dc89b0>,\n",
       " 'visual_size': <absl.flags._flag.Flag at 0x7f8363dc81d0>,\n",
       " 'n_samples': <absl.flags._flag.Flag at 0x7f8363dc8048>,\n",
       " 'print_every': <absl.flags._flag.Flag at 0x7f8363dc8780>,\n",
       " 'hidden_size': <absl.flags._flag.Flag at 0x7f7f18da2f98>,\n",
       " 'n_iterations': <absl.flags._flag.Flag at 0x7f7f18da2cc0>,\n",
       " 'threshold': <absl.flags._flag.Flag at 0x7f7f18da2780>}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FLAGS._flags() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_network(x, latent_dim, hidden_size , p):\n",
    "    \"\"\"Construct an inference network parametrizing a Gaussian.\n",
    "        Args:\n",
    "        x: A batch of MNIST digits.\n",
    "        latent_dim: The latent dimensionality.\n",
    "        hidden_size: The size of the neural net hidden layers.\n",
    "\n",
    "      Returns:\n",
    "        mu: Mean parameters for the variational family Normal\n",
    "        sigma: Standard deviation parameters for the variational family Normal\n",
    "     \"\"\"\n",
    "    with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.leaky_relu):\n",
    "        net = slim.flatten(x)\n",
    "        net = slim.fully_connected(net, hidden_size )\n",
    "        net = slim.fully_connected(net, hidden_size )\n",
    "#         net = tf.contrib.layers.batch_norm(net, center=True, scale=True, \n",
    "#                                           is_training= p ,\n",
    "#                                           scope='generative_bn')\n",
    "#         net = slim.fully_connected(net, hidden_size )\n",
    "#         net = tf.contrib.layers.batch_norm(net, center=True, scale=True , \n",
    "#                                           is_training= p ,\n",
    "#                                           scope='generative_bn2')\n",
    "        net = slim.fully_connected(net, hidden_size  )\n",
    "        gaussian_params = slim.fully_connected(net, latent_dim * 2, activation_fn=None)\n",
    "  # The mean parameter is unconstrained\n",
    "    mu = gaussian_params[:, :latent_dim]\n",
    "  # The standard deviation must be positive. Parametrize with a softplus\n",
    "    sigma = tf.nn.softplus(gaussian_params[:, latent_dim:])\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generative_network(z, hidden_size , p):\n",
    "    \"\"\"Build a generative network parametrizing the likelihood of the data\n",
    "\n",
    "  Args:\n",
    "    z: Samples of latent variables\n",
    "    hidden_size: Size of the hidden state of the neural net\n",
    "\n",
    "  Returns:\n",
    "    bernoulli_logits: logits for the Bernoulli likelihood of the data\n",
    "  \"\"\"\n",
    "    with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.leaky_relu):\n",
    "        net = slim.fully_connected(z, hidden_size )\n",
    "        #net = tf.nn.dropout(net, p )\n",
    "#         net = tf.contrib.layers.batch_norm(net, center=True, scale=True, \n",
    "#                                           is_training=p,\n",
    "#                                           scope='inference_bn')\n",
    "#         net = slim.fully_connected(net, hidden_size )\n",
    "#         net = tf.contrib.layers.batch_norm(net, center=True, scale=True, \n",
    "#                                           is_training= p ,\n",
    "#                                           scope='inference_bn2')\n",
    "        net = slim.fully_connected(net, hidden_size )\n",
    "        net = slim.fully_connected(net, hidden_size )\n",
    "        net = slim.fully_connected(net, hidden_size )\n",
    "        \n",
    "        bernoulli_logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "        bernoulli_logits = tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n",
    "        \n",
    "    return bernoulli_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  # Train a Variational Autoencoder on MNIST\n",
    "\n",
    "  # Input placeholders\n",
    "    prob = tf.placeholder(tf.bool, name =\"bn_bool\")\n",
    "    with tf.name_scope('data'):\n",
    "        x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        tf.summary.image('data', x)\n",
    "    with tf.variable_scope('variational'):\n",
    "        q_mu, q_sigma = inference_network(x=x,\n",
    "                                          latent_dim=FLAGS.latent_dim,\n",
    "                                          hidden_size=FLAGS.hidden_size , \n",
    "                                          p = prob)\n",
    "        \n",
    "    # The variational distribution is a Normal with mean and standard\n",
    "    # deviation given by the inference network\n",
    "    q_z = distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "    assert q_z.reparameterization_type == distributions.FULLY_REPARAMETERIZED\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "    # The likelihood is Bernoulli-distributed with logits given by the\n",
    "    # generative network\n",
    "        mySample = q_z.sample()\n",
    "        p_x_given_z_logits = generative_network(z=mySample,\n",
    "                                                hidden_size=FLAGS.hidden_size , \n",
    "                                                p = prob)\n",
    "        p_x_given_z = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "        posterior_predictive_samples = p_x_given_z.sample()\n",
    "        tf.summary.image('posterior_predictive',\n",
    "                         tf.cast(posterior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior\n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "        p_z = distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                                   scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "        p_z_sample = p_z.sample(FLAGS.n_samples)\n",
    "        p_x_given_z_logits = generative_network(z=p_z_sample,\n",
    "                                                hidden_size=FLAGS.hidden_size , \n",
    "                                                p = prob)\n",
    "    prior_predictive = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_samples = prior_predictive.sample()\n",
    "    tf.summary.image('prior_predictive',\n",
    "                     tf.cast(prior_predictive_samples, tf.float32))\n",
    "\n",
    "#   # Take samples from the prior with a placeholder\n",
    "    with tf.variable_scope('model', reuse=True):\n",
    "        z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
    "        p_x_given_z_logits = generative_network(z=z_input,\n",
    "                                                hidden_size=FLAGS.hidden_size , \n",
    "                                                p = prob)\n",
    "        prior_predictive_inp = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "        prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
    "\n",
    "  # Build the evidence lower bound (ELBO) or the negative loss\n",
    "    kl = tf.reduce_sum(distributions.kl_divergence(q_z, p_z), 1)\n",
    "#   tense = p_x_given_z.log_prob(x)\n",
    "    expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n",
    "                                          [1, 2, 3])\n",
    "    #expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),[1, 2, 3])\n",
    "  # print \"p_x_given_z.log_prob(x).shape\", p_x_given_z.log_prob(x).shape\n",
    "  # shapeTensor =  tf.shape(p_x_given_z.log_prob(x))\n",
    "\n",
    "    #elbo = -tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "     \n",
    "    #marginal_likelihood = tf.reduce_mean(expected_log_likelihood)\n",
    "    #KL_divergence = tf.reduce_mean(kl)\n",
    "    #elbo = - (marginal_likelihood - KL_divergence ) \n",
    "    elbo = -tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = optimizer.minimize( elbo )\n",
    "    summary_op = tf.summary.merge_all() ## # Merge all the summaries\n",
    "    init_op = tf.global_variables_initializer()\n",
    "  # Run training\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(init_op)\n",
    "    save_file = \"./../../Save_Model/VAE_OD.meta\"\n",
    "    try :\n",
    "        saver = tf.train.import_meta_graph(save_file)\n",
    "        saver.restore(sess, tf.train.latest_checkpoint('./../../Save_Model/'))\n",
    "    except Exception as e :\n",
    "        pass\n",
    "    mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "    train, test = mnist.train, mnist.test\n",
    "    trainX, trainY, testX, testY = train.images, train.labels, test.images, test.labels\n",
    "    ### 0만??? \n",
    "    k = 0\n",
    "    print(trainY[0:5])\n",
    "    mask = trainY[:,k]==0 # omit all examples of the class k  0을 제외하고 1~9만 뽑기\n",
    "    print(mask[0:5])\n",
    "    trainX = trainX[mask]\n",
    "    trainY = trainY[mask]\n",
    "    n = trainX.shape[0]\n",
    "    print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
    "    train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "  # Get fixed MNIST digits for plotting posterior means during training\n",
    "    t0 = time.time()\n",
    "    cut_line = 1.0 * 80 # FLAGS.batch_size\n",
    "    for i in range(FLAGS.n_iterations):\n",
    "        batch_idx = np.arange(len(trainX))\n",
    "        np.random.shuffle(batch_idx)\n",
    "        if i % 100 == 0 :\n",
    "            print(\"Epoch : \", i)\n",
    "        for n in range(0, ( len(trainX) // FLAGS.batch_size) * FLAGS.batch_size, FLAGS.batch_size) :\n",
    "            batch = batch_idx[n: n + FLAGS.batch_size]\n",
    "        # Re-binarize the data at every batch; this improves results\n",
    "#     np_x, _ = train.next_batch(FLAGS.batch_size)\n",
    "            np_x = trainX[batch,:]\n",
    "            np_y = trainY[batch,:]\n",
    "    #     print(np_x.shape)\n",
    "    #     np_x += np.random.randn(np_x.shape)\n",
    "            np_x = np_x.reshape(FLAGS.batch_size, 28, 28, 1)\n",
    "            np_x = (np_x > 0.5).astype(np.float32)\n",
    "        #     print np_x.shape\n",
    "            sess.run(train_op, {x: np_x , prob : 1 })\n",
    "#     print samples.shape\n",
    "    # Print progress and save samples every so often\n",
    "        if i % FLAGS.print_every == 0:\n",
    "            np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x , prob : 0})\n",
    "            train_writer.add_summary(summary_str, i)\n",
    "            print('Iteration: {0:d} ELBO: {1:.3f} s/iter: {2:.3e}'.format(\n",
    "                i, np_elbo / FLAGS.batch_size, (time.time() - t0) / FLAGS.print_every))\n",
    "            if np_elbo / FLAGS.batch_size < cut_line : \n",
    "                print(\" {} -> {}\".format(cut_line , np_elbo / FLAGS.batch_size ))\n",
    "                saver.save(sess, './../../Save_Model/VAE_OD')\n",
    "                cut_line = np_elbo / FLAGS.batch_size\n",
    "                \n",
    "            t0 = time.time() \n",
    "          # Save samples\n",
    "            np_posterior_samples, np_prior_samples = sess.run(\n",
    "                [posterior_predictive_samples, prior_predictive_samples], {x: np_x , prob : 0 })\n",
    "            for k in range(FLAGS.n_samples):\n",
    "                f_name = os.path.join(\n",
    "                    FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
    "                imsave(f_name, np_x[k, :, :, 0])\n",
    "                f_name = os.path.join(\n",
    "                    FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
    "                imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
    "                f_name = os.path.join(\n",
    "                    FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
    "                imsave(f_name, np_prior_samples[k, :, :, 0])\n",
    "\n",
    "          # Plot the posterior predictive space\n",
    "            if FLAGS.latent_dim == 2:\n",
    "                visual_n = np.floor(np.random.rand(FLAGS.visual_size)*n).astype(int)\n",
    "                vis_x = trainX[visual_n,:]\n",
    "                vis_y = trainY[visual_n,:]\n",
    "        #     print(np_x.shape)\n",
    "        #     np_x += np.random.randn(np_x.shape)\n",
    "                vis_x = vis_x.reshape(FLAGS.visual_size, 28, 28, 1)\n",
    "                vis_x = (vis_x > 0.5).astype(np.float32)\n",
    "                np_q_mu = sess.run(q_mu, {x: vis_x , prob : 0 })\n",
    "                cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\" , 10))\n",
    "                f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
    "                print(np.shape(np_q_mu))\n",
    "                print(np.shape(vis_y))\n",
    "                im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(vis_y, 1),\n",
    "                                cmap=cmap, alpha=0.7)\n",
    "                ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
    "                ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
    "                ax.set_xlim([-10., 10.])\n",
    "                ax.set_ylim([-10., 10.])\n",
    "                f.colorbar(im, ax=ax, label='Digit class')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(FLAGS.logdir,\n",
    "                                         'posterior_predictive_map_frame_%d.png' % i))\n",
    "                plt.close()\n",
    "\n",
    "                nx = ny = 20\n",
    "                x_values = np.linspace(-3, 3, nx)\n",
    "                y_values = np.linspace(-3, 3, ny)\n",
    "                canvas = np.empty((28 * ny, 28 * nx))\n",
    "                for ii, yi in enumerate(x_values):\n",
    "                    for j, xi in enumerate(y_values):\n",
    "                        np_z = np.array([[xi, yi]])\n",
    "                        x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z , prob : 0})\n",
    "                        canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
    "                           28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
    "                imsave(os.path.join(FLAGS.logdir,'prior_predictive_map_frame_%d.png' % i), canvas)\n",
    "#                 plt.figure(figsize=(8, 10))\n",
    "#                 Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "#                 plt.imshow(canvas, origin=\"upper\")\n",
    "#                 plt.tight_layout()\n",
    "                #plt.savefig()\n",
    "    # Make the gifs 다 학습시키고 gif 만들기?\n",
    "    if FLAGS.latent_dim == 2:\n",
    "        print(\"Image -> Gif\")\n",
    "        os.system('convert -delay 15 -loop 0 {0}/posterior_predictive_map_frame*png {0}/posterior_predictive.gif'\n",
    "                  .format(FLAGS.logdir))\n",
    "        os.system('convert -delay 15 -loop 0 {0}/prior_predictive_map_frame*png {0}/prior_predictive.gif'\n",
    "                  .format(FLAGS.logdir))\n",
    "    ## 이놈은 그냥 샘플 뽑기 \n",
    "    np_x_fixed, np_y = test.next_batch(5000)\n",
    "    np_x_fixed = np_x_fixed.reshape(5000, 28, 28, 1)\n",
    "    np_x_fixed = (np_x_fixed > 0.5).astype(np.float32)\n",
    "    ## class가 0인놈만 뽑기 (outlier같은 느낌이 되는거겠지?) \n",
    "    np_y = np.asarray(np_y[:,k]==1, dtype=np.float32)\n",
    "  # evaluate the test samples 특정 기준으로만?!\n",
    "    np_posterior_samples, np_prior_samples = sess.run(\n",
    "        [posterior_predictive_samples, prior_predictive_samples], {x: np_x_fixed  , prob : 0 })\n",
    "    ## 이것의 의미는 뭘까? \n",
    "    norm_vec = np.linalg.norm((np_x_fixed - np_posterior_samples).reshape(5000,784) , axis=1)\n",
    "#   np_prior_samples[k, :, :, 0] \n",
    "    ## 특정 threshold 보다 크면 TRUE라고 내뱉기\n",
    "    ## 학습시에는 0을 뺴고 한 상태 여기는 0이 나올 수도 있는 상태\n",
    "    ## 그러므로 만약 이 놈이 0을 잘 탐지했다면 norm_vec 이 크게 나올 것이다. \n",
    "    anomaly_vector = np.asarray(norm_vec > FLAGS.threshold, np.float32)\n",
    "    ## mse가 커서 outlier라고 예측\n",
    "    y_pred = anomaly_vector\n",
    "    ## 실제 타켓에서 0인 것에 대한 T/F\n",
    "    y_real = np_y\n",
    "    val_accuracy = np.sum(y_pred == y_real)*1.0/5000.0\n",
    "#   y_truth = tf.placeholder(tf.float32, [5000,])\n",
    "#   corr_pred = tf.equal(y_p, y_truth)\n",
    "#   accuracy = tf.reduce_mean(tf.cast(corr_pred, \"float\"))\n",
    "#   val_accuracy, y_pred, rp = sess.run([accuracy, y_p, expected_log_likelihood], feed_dict={x:np_x_fixed, y_truth:np_y})\n",
    "    outlier_idx = np_y == 1\n",
    "    normal_idx = np_y == 0\n",
    "    normal_data = norm_vec[normal_idx]\n",
    "    outlier_data = norm_vec[outlier_idx] \n",
    "    plt.hist(normal_data, label= \"Normal\")\n",
    "    plt.hist(outlier_data, label= \"Outlier\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    print(\"validation accuracy:\", val_accuracy)\n",
    "#   y_true = np.asarray(np.argmax(np_y,1), np.float32)\n",
    "#   print y_real, y_pred\n",
    "    print(np.sum(y_pred)) ## 이상하다고 말하는 놈들의 수\n",
    "    print(np.sum(y_real)) ## 실제 정상의 수 \n",
    "    print(\"Precision\", sk.metrics.precision_score(y_real, y_pred))\n",
    "    print(\"Recall\", sk.metrics.recall_score(y_real, y_pred))\n",
    "    print(\"f1_score\", sk.metrics.f1_score(y_real, y_pred))\n",
    "    print(\"confusion_matrix\")\n",
    "    print(sk.metrics.confusion_matrix(y_real, y_pred)  )\n",
    "    fpr, tpr, thresholds = sk.metrics.roc_curve(y_real, y_pred)\n",
    "    auc = sk.metrics.roc_auc_score(y_real, y_pred)\n",
    "    print(\"{} , AUC={:.3f}\".format(i , auc))\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.logdir):\n",
    "        ## 전부다 제거하기\n",
    "        tf.gfile.DeleteRecursively(FLAGS.logdir)\n",
    "        ## 폴더 새로 생성하는 것 같음\n",
    "        tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "        train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[ True  True  True  True  True]\n",
      "Saving TensorBoard summaries and images to: /home/advice/Python/SR/board/VAE_OD\n",
      "Epoch :  0\n",
      "Iteration: 0 ELBO: 156.962 s/iter: 1.612e-02\n",
      "(1000, 2)\n",
      "(1000, 10)\n",
      "Epoch :  100\n",
      "Iteration: 100 ELBO: 127.894 s/iter: 1.227e+00\n",
      "(1000, 2)\n",
      "(1000, 10)\n",
      "Epoch :  200\n",
      "Iteration: 200 ELBO: 120.930 s/iter: 1.217e+00\n",
      "(1000, 2)\n",
      "(1000, 10)\n",
      "Epoch :  300\n",
      "Iteration: 300 ELBO: 119.005 s/iter: 1.229e+00\n",
      "(1000, 2)\n",
      "(1000, 10)\n",
      "Epoch :  400\n",
      "Iteration: 400 ELBO: 121.274 s/iter: 1.229e+00\n",
      "(1000, 2)\n",
      "(1000, 10)\n",
      "Epoch :  500\n",
      "Iteration: 500 ELBO: 115.848 s/iter: 1.212e+00\n",
      "(1000, 2)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
